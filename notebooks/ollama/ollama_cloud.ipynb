{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to connect to your local Ollama instance.\n",
    "\n",
    "## Prerequisites\n",
    "- Ollama installed and running locally\n",
    "- Python ollama package installed\n",
    "- At least one model pulled (e.g., gpt-oss:20b, gpt-oss:120b, etc.)\n",
    "\n",
    "##  Approaches Covered:\n",
    "cloud - High-performance cloud processing (requires subscription)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "**Note:** Make sure you have the required packages installed:\n",
    "```bash\n",
    "pip install ollama python-dotenv\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import ollama\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load configuration from environment variables\n",
    "OLLAMA_CLOUD_BASE_URL = os.getenv('OLLAMA_CLOUD_BASE_URL', 'https://ollama.com')\n",
    "OLLAMA_CLOUD_MODEL = os.getenv('OLLAMA_CLOUD_MODEL', 'gpt-oss:20b')\n",
    "OLLAMA_API_KEY = os.getenv('OLLAMA_API_KEY', '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Ollama Cloud Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Successfully connected to Ollama Cloud!\n",
      "Connection time: 2025-10-01 19:38:36\n",
      "Connected to: https://ollama.com\n",
      "Cloud Model: gpt-oss:20b\n",
      "API Key: Set\n",
      "\n",
      "Testing cloud model availability...\n",
      "SUCCESS: Cloud model 'gpt-oss:20b' is working!\n"
     ]
    }
   ],
   "source": [
    "# Test connection to Ollama cloud service\n",
    "if not OLLAMA_API_KEY:\n",
    "    print(\"ERROR: Ollama API key not found in .env file!\")\n",
    "else:\n",
    "    try:\n",
    "        # Test cloud connection\n",
    "        cloud_client = ollama.Client(\n",
    "            host=OLLAMA_CLOUD_BASE_URL,\n",
    "            headers={'Authorization': f'Bearer {OLLAMA_API_KEY}'}\n",
    "        )\n",
    "        \n",
    "        print(\"SUCCESS: Successfully connected to Ollama Cloud!\")\n",
    "        print(f\"Connection time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Connected to: {OLLAMA_CLOUD_BASE_URL}\")\n",
    "        print(f\"Cloud Model: {OLLAMA_CLOUD_MODEL}\")\n",
    "        print(f\"API Key: {'Set' if OLLAMA_API_KEY else 'Not set'}\")\n",
    "        \n",
    "        # request\n",
    "        print(\"\\nTesting cloud model availability...\")\n",
    "        try:\n",
    "            response = cloud_client.generate(\n",
    "                model=OLLAMA_CLOUD_MODEL,\n",
    "                prompt=\"Hello\",\n",
    "                options={'num_predict': 10}\n",
    "            )\n",
    "            print(f\"SUCCESS: Cloud model '{OLLAMA_CLOUD_MODEL}' is working!\")\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING: Cloud model test failed: {e}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to connect to Ollama Cloud: {e}\")\n",
    "        print(\"\\nTroubleshooting tips:\")\n",
    "        print(\"1. Check your API key in the .env file\")\n",
    "        print(\"2. Verify your subscription at https://ollama.com\")\n",
    "        print(\"3. Check your internet connection\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cloud Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "Artificial intelligence is the simulation of human intelligence processes by machines—particularly computer systems—that enable them to learn, reason, and solve problems\n",
      "\n",
      "Generation time: 1.40 seconds\n",
      "Response length: 169 characters\n",
      "Tokens per second: 15.0\n"
     ]
    }
   ],
   "source": [
    "# Configure model for cloud usage using environment variables\n",
    "MODEL_NAME = OLLAMA_CLOUD_MODEL  # Use cloud model from .env file\n",
    "\n",
    "try:\n",
    "    # Create cloud client\n",
    "    cloud_client = ollama.Client(\n",
    "        host=OLLAMA_CLOUD_BASE_URL,\n",
    "        headers={'Authorization': f'Bearer {OLLAMA_API_KEY}'}\n",
    "    )\n",
    "    \n",
    "    # Measure generation time using time.time()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Basic generation with cloud model\n",
    "    response = cloud_client.generate(\n",
    "        model=MODEL_NAME,\n",
    "        prompt=\"Explain what artificial intelligence is in one sentence.\",\n",
    "        options={\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.9,\n",
    "            'num_predict': 100\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    generation_time = end_time - start_time\n",
    "    \n",
    "    print(\"Response:\")\n",
    "    print(response['response'])\n",
    "    print(f\"\\nGeneration time: {generation_time:.2f} seconds\")\n",
    "    print(f\"Response length: {len(response['response'])} characters\")\n",
    "    print(f\"Tokens per second: {len(response['response'].split()) / generation_time:.1f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Error with cloud model {MODEL_NAME}: {e} , make sure your API key is set in the .env file\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming Responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "In loops we chase the logic's dance,  \n",
      "A cursor glides through endless code.  \n",
      "Variables whisper, functions rhyme—  \n",
      "The silent hum of keys, our ode.\n",
      "\n",
      "SUCCESS: Streaming completed!\n",
      "Total tokens: 24 words\n"
     ]
    }
   ],
   "source": [
    "# streaming\n",
    "cloud_client = ollama.Client(\n",
    "    host=OLLAMA_CLOUD_BASE_URL,\n",
    "    headers={'Authorization': f'Bearer {OLLAMA_API_KEY}'}\n",
    "    )\n",
    "\n",
    "stream = cloud_client.generate(\n",
    "    model=MODEL_NAME,\n",
    "    prompt=\"Write a short poem about programming.\",\n",
    "    stream=True,\n",
    "    options={\n",
    "        'temperature': 0.8,\n",
    "        'num_predict': 150\n",
    "        }\n",
    "    )\n",
    "    \n",
    "print(\"Streaming response:\")\n",
    "full_response = \"\"\n",
    "\n",
    "for chunk in stream:\n",
    "    if 'response' in chunk:\n",
    "        print(chunk['response'], end='', flush=True)\n",
    "        full_response += chunk['response']\n",
    "\n",
    "print(\"\\n\\nSUCCESS: Streaming completed!\")\n",
    "print(f\"Total tokens: {len(full_response.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This section demonstrates the cloud-based Ollama Turbo service, which requires:\n",
    "- Ollama account subscription ($20/month)\n",
    "- API key from https://ollama.com/settings/keys\n",
    "- Internet connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found - testing Turbo cloud service...\n",
      "SUCCESS: Ollama Turbo client configured successfully!\n",
      "Cloud URL: https://ollama.com\n",
      "Cloud Model: gpt-oss:20b\n",
      "API Key: Set\n",
      "\n",
      "Testing Ollama Turbo Cloud Service\n",
      "==================================================\n",
      "Streaming response from Turbo cloud (gpt-oss:20b):\n",
      "### What is quantum computing?\n",
      "\n",
      "Think of a normal computer as a very fast **dumb switch** that can flip a bit from **0** to **1** (or vice‑versa). Every calculation is a sequence of those on/off switches. All the work an ordinary computer can do is done by turning the right mixture of switches on and off at the right times.\n",
      "\n",
      "A quantum computer, on the other hand, uses *quantum bits* (**qubits**) that can be 0, 1, or a **blend of both** at the same time. This blend is called **superposition**. It’s like having a coin that can be heads, tails, *and* in between all at once. Because of this, a handful of qubits can represent many more possible states at the same moment than the same number of regular bits.\n",
      "\n",
      "### Two key ingredients\n",
      "\n",
      "1. **Superposition** – A qubit can hold a 0 and a 1 simultaneously, so N qubits can represent 2^N different combinations all at once.  \n",
      "2. **Entanglement** – When qubits become entangled, the state of one instantly influences the state of another, no matter how far apart they are. This links their possibilities together in ways classical bits never can.\n",
      "\n",
      "### Why does that matter?\n",
      "\n",
      "Because a quantum computer can process a huge number of combinations in parallel, it can solve certain kinds of problems—like factoring huge numbers, searching massive databases, or simulating molecules—much faster than any classical computer. Think of it as having a super‑efficient “all‑at‑once” switchboard.\n",
      "\n",
      "### A simple analogy\n",
      "\n",
      "Imagine you’re looking for a name in a huge library.  \n",
      "* **Classical method**: open one book at a time, page by page.  \n",
      "* **Quantum method**: open *all* books *simultaneously*, and your “quantum search” highlights the correct name instantly.\n",
      "\n",
      "### In practice\n",
      "\n",
      "We’re still in the early days. Current quantum machines have a few dozen or a few hundred qubits, but they are noisy and fragile. Yet already, researchers can solve toy problems that would be practically impossible for today's best supercomputers.\n",
      "\n",
      "---\n",
      "\n",
      "**Bottom line:** A quantum computer stores and processes information in a way that uses the weird rules of quantum physics, allowing it to handle many possibilities at once and solve some special problems much faster than classical computers.\n",
      "\n",
      "Total time: 2.75 seconds\n",
      "SUCCESS: Turbo cloud service test completed!\n"
     ]
    }
   ],
   "source": [
    "def setup_turbo_client():\n",
    "    if not OLLAMA_API_KEY:\n",
    "        print(\"ERROR: Ollama API key not found in .env file!\")\n",
    "        return None\n",
    "    try:\n",
    "        turbo_client = ollama.Client(\n",
    "            host=OLLAMA_CLOUD_BASE_URL,\n",
    "            headers={'Authorization': f'Bearer {OLLAMA_API_KEY}'}\n",
    "        )\n",
    "        print(\"SUCCESS: Ollama Turbo client configured successfully!\")\n",
    "        print(f\"Cloud URL: {OLLAMA_CLOUD_BASE_URL}\")\n",
    "        print(f\"Cloud Model: {OLLAMA_CLOUD_MODEL}\")\n",
    "        print(f\"API Key: {'Set' if OLLAMA_API_KEY else 'Not set'}\")\n",
    "        return turbo_client\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error setting up Turbo client: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_turbo_performance():\n",
    "    turbo_client = setup_turbo_client()\n",
    "    if not turbo_client:\n",
    "        return\n",
    "    \n",
    "    print(\"\\nTesting Ollama Turbo Cloud Service\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        messages = [\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': 'Explain quantum computing in simple terms.',\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        print(f\"Streaming response from Turbo cloud ({OLLAMA_CLOUD_MODEL}):\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for part in turbo_client.chat(OLLAMA_CLOUD_MODEL, messages=messages, stream=True):\n",
    "            print(part['message']['content'], end='', flush=True)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"\\n\\nTotal time: {end_time - start_time:.2f} seconds\")\n",
    "        print(\"SUCCESS: Turbo cloud service test completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Turbo cloud error: {e}\")\n",
    "\n",
    "#  API key is available -> run test, else add API key to .env file\n",
    "if OLLAMA_API_KEY:\n",
    "    print(\"API key found - testing Turbo cloud service...\")\n",
    "    test_turbo_performance()\n",
    "else:\n",
    "    print(\"Add OLLAMA_API_KEY to your .env file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Output to Markdown File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown output functionality\n",
    "class MarkdownWriter:\n",
    "    \"\"\"Class to write outputs to markdown files.\"\"\"\n",
    "    \n",
    "    def __init__(self, filename=\"ollama_output.md\"):\n",
    "        # Ensure outputs directory exists\n",
    "        outputs_dir = Path(\"outputs\")\n",
    "        outputs_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Set filename to outputs subfolder\n",
    "        self.filename = outputs_dir / filename\n",
    "        self.content = []\n",
    "        self.start_time = datetime.now()\n",
    "        \n",
    "    def add_header(self, level, text):\n",
    "        \"\"\"Add a header to the markdown.\"\"\"\n",
    "        self.content.append(f\"{'#' * level} {text}\\n\")\n",
    "        \n",
    "    def add_text(self, text):\n",
    "        \"\"\"Add plain text to the markdown.\"\"\"\n",
    "        self.content.append(f\"{text}\\n\")\n",
    "        \n",
    "    def add_code_block(self, code, language=\"python\"):\n",
    "        \"\"\"Add a code block to the markdown.\"\"\"\n",
    "        self.content.append(f\"```{language}\\n{code}\\n```\\n\")\n",
    "        \n",
    "    def add_json_block(self, data, title=\"JSON Data\"):\n",
    "        \"\"\"Add formatted JSON to the markdown.\"\"\"\n",
    "        self.content.append(f\"### {title}\\n\")\n",
    "        self.content.append(f\"```json\\n{json.dumps(data, indent=2)}\\n```\\n\")\n",
    "        \n",
    "    def add_table(self, headers, rows):\n",
    "        \"\"\"Add a table to the markdown.\"\"\"\n",
    "        # Create table header\n",
    "        header_row = \"| \" + \" | \".join(headers) + \" |\"\n",
    "        separator = \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\"\n",
    "        \n",
    "        self.content.append(header_row + \"\\n\")\n",
    "        self.content.append(separator + \"\\n\")\n",
    "        \n",
    "        # Add data rows\n",
    "        for row in rows:\n",
    "            row_str = \"| \" + \" | \".join(str(cell) for cell in row) + \" |\"\n",
    "            self.content.append(row_str + \"\\n\")\n",
    "        \n",
    "    def add_timestamp(self):\n",
    "        \"\"\"Add timestamp to the markdown.\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        self.content.append(f\"\\n---\\n*Generated on: {timestamp}*\\n\")\n",
    "        \n",
    "    def save(self):\n",
    "        \"\"\"Save the markdown content to file.\"\"\"\n",
    "        # Add header with timestamp\n",
    "        self.content.insert(0, f\"# Ollama Turbo Cloud Service Report\\n\")\n",
    "        self.content.insert(1, f\"*Generated on: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}*\\n\\n\")\n",
    "        \n",
    "        # Write to file\n",
    "        with open(self.filename, 'w', encoding='utf-8') as f:\n",
    "            f.writelines(self.content)\n",
    "        \n",
    "        print(f\"SUCCESS: Markdown output saved to {self.filename}\")\n",
    "        return self.filename\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Turbo cloud service markdown output...\n",
      "SUCCESS: Markdown output saved to outputs/ollama_turbo_output.md\n",
      "Output saved to: outputs/ollama_turbo_output.md\n",
      "\n",
      "Preview of outputs/ollama_turbo_output.md:\n",
      "==================================================\n",
      "# Ollama Turbo Cloud Service Report\n",
      "*Generated on: 2025-10-01 19:38:43*\n",
      "\n",
      "## Configuration\n",
      "Current Ollama Turbo cloud configuration loaded from .env file:\n",
      "### Turbo Cloud Configuration\n",
      "```json\n",
      "{\n",
      "  \"cloud_url\": \"https://ollama.com\",\n",
      "  \"cloud_model\": \"gpt-oss:20b\",\n",
      "  \"api_key_set\": true\n",
      "}\n",
      "```\n",
      "## Turbo Cloud Service Test\n",
      "Testing Ollama Turbo Cloud Service performance...\n",
      "**Model:** gpt-oss:20b\n",
      "**Prompt:** Explain quantum computing in simple terms.\n",
      "**Response:**\n",
      "## Quantum Computing – In Plain English...\n"
     ]
    }
   ],
   "source": [
    "# Example usage of MarkdownWriter with Turbo cloud service\n",
    "def ymo_markdown_output():\n",
    "    # Create markdown writer\n",
    "    md_writer = MarkdownWriter(\"ollama_turbo_output.md\")\n",
    "    \n",
    "    # Add configuration section\n",
    "    md_writer.add_header(2, \"Configuration\")\n",
    "    md_writer.add_text(\"Current Ollama Turbo cloud configuration loaded from .env file:\")\n",
    "    \n",
    "    config_data = {\n",
    "        \"cloud_url\": OLLAMA_CLOUD_BASE_URL,\n",
    "        \"cloud_model\": OLLAMA_CLOUD_MODEL,\n",
    "        \"api_key_set\": bool(OLLAMA_API_KEY)\n",
    "    }\n",
    "    \n",
    "    md_writer.add_json_block(config_data, \"Turbo Cloud Configuration\")\n",
    "    \n",
    "    # service test section\n",
    "    md_writer.add_header(2, \"Turbo Cloud Service Test\")\n",
    "    \n",
    "    if OLLAMA_API_KEY:\n",
    "        try:\n",
    "            # Setup Turbo client\n",
    "            turbo_client = ollama.Client(\n",
    "                host=OLLAMA_CLOUD_BASE_URL,\n",
    "                headers={'Authorization': f'Bearer {OLLAMA_API_KEY}'}\n",
    "            )\n",
    "            \n",
    "            md_writer.add_text(\"Testing Ollama Turbo Cloud Service performance...\")\n",
    "            \n",
    "            messages = [\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': 'Explain quantum computing in simple terms.',\n",
    "                },\n",
    "            ]\n",
    "            \n",
    "            md_writer.add_text(f\"**Model:** {OLLAMA_CLOUD_MODEL}\")\n",
    "            md_writer.add_text(f\"**Prompt:** Explain quantum computing in simple terms.\")\n",
    "            md_writer.add_text(\"**Response:**\")\n",
    "            \n",
    "            # Capture streaming response + time\n",
    "            start_time = time.time()\n",
    "            full_response = \"\"\n",
    "            \n",
    "            for part in turbo_client.chat(OLLAMA_CLOUD_MODEL, messages=messages, stream=True):\n",
    "                if 'message' in part and 'content' in part['message']:\n",
    "                    content = part['message']['content']\n",
    "                    full_response += content\n",
    "            \n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            # response to markdown\n",
    "            md_writer.add_text(full_response)\n",
    "            md_writer.add_text(f\"\\n**Response Time:** {duration:.2f} seconds\")\n",
    "            md_writer.add_text(f\"**Response Length:** {len(full_response)} characters\")\n",
    "            \n",
    "            # Add performance summary\n",
    "            performance_summary = {\n",
    "                \"model\": OLLAMA_CLOUD_MODEL,\n",
    "                \"response_time_seconds\": round(duration, 2),\n",
    "                \"response_length\": len(full_response),\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "            \n",
    "            md_writer.add_json_block(performance_summary, \"Performance Summary\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            md_writer.add_text(f\"Error during Turbo cloud service test: {e}\")\n",
    "            error_info = {\n",
    "                \"error\": str(e),\n",
    "                \"status\": \"failed\"\n",
    "            }\n",
    "            md_writer.add_json_block(error_info, \"Error Details\")\n",
    "    else:\n",
    "        md_writer.add_text(\"API key not configured - cannot test Turbo cloud service\")\n",
    "        md_writer.add_text(\"To use Ollama Turbo cloud service:\")\n",
    "        md_writer.add_text(\"1. Sign up at https://ollama.com ($20/month)\")\n",
    "        md_writer.add_text(\"2. Get API key from https://ollama.com/settings/keys\")\n",
    "        md_writer.add_text(\"3. Add OLLAMA_API_KEY to your .env file\")\n",
    "    \n",
    "    # Add end of report blah blah \n",
    "    md_writer.add_header(2, \"Summary\")\n",
    "    md_writer.add_text(\"This report demonstrates Ollama Turbo Cloud Service integration with:\")\n",
    "    md_writer.add_text(\"- Environment configuration loading\")\n",
    "    md_writer.add_text(\"- Turbo cloud service testing\")\n",
    "    md_writer.add_text(\"- Streaming response capture\")\n",
    "    md_writer.add_text(\"- Performance metrics collection\")\n",
    "    md_writer.add_text(\"- Markdown output generation\")\n",
    "    \n",
    "    # Save\n",
    "    filename = md_writer.save()\n",
    "    return filename\n",
    "\n",
    "# Run \n",
    "print(\"Generating Turbo cloud service markdown output...\")\n",
    "output_file = ymo_markdown_output()\n",
    "print(f\"Output saved to: {output_file}\")\n",
    "\n",
    "# Content\n",
    "if Path(output_file).exists():\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        print(f\"\\nPreview of {output_file}:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(content[:500] + \"...\" if len(content) > 500 else content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "- [Ollama Documentation](https://ollama.ai/docs)\n",
    "- [Ollama Python Package](https://github.com/ollama/ollama-python)\n",
    "- [Model Library](https://ollama.ai/library)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
