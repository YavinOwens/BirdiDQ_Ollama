{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What This Notebook Accomplishes:\n",
        "\n",
        "1. **Data Processing**: Loads and processes Great Expectations validation JSON files\n",
        "2. **Analytics**: Calculates comprehensive data quality metrics and trends\n",
        "3. **Visualizations**: Creates both static and interactive charts and graphs\n",
        "4. **AI Analysis**: Uses Ollama LLM (gpt-oss:20b) for intelligent insights\n",
        "5. **Report Generation**: Exports professional markdown and PDF reports\n",
        "\n",
        "### Usage Instructions:\n",
        "\n",
        "1. **Run All Cells**: Execute the entire notebook to generate the complete analysis\n",
        "2. **Customize**: Modify the `ValidationAnalyzer` class for different data sources\n",
        "3. **Schedule**: Set up automated runs for regular data quality monitoring\n",
        "4. **Share**: Use generated reports for stakeholder communication\n",
        "\n",
        "### Dependencies:\n",
        "\n",
        "- **Core**: pandas, numpy, matplotlib, seaborn, plotly\n",
        "- **AI**: requests (for Ollama API)\n",
        "- **PDF Export**: weasyprint, markdown (optional)\n",
        "- **Data Source**: Great Expectations validation results\n",
        "\n",
        " ### Generated Outputs:\n",
        "\n",
        "- **Interactive Dashboard**: Plotly visualizations for trend analysis\n",
        "- **Static Charts**: Matplotlib/Seaborn charts for presentations\n",
        "- **Markdown Report**: Comprehensive analysis report\n",
        "- **PDF Report**: Professional formatted document (requires weasyprint)\n",
        "\n",
        " ### Key Features:\n",
        "\n",
        "- **Automated Analysis**: Processes all validation files automatically\n",
        "- **Trend Detection**: Identifies patterns and trends over time\n",
        "- **AI Insights**: Leverages Ollama for intelligent recommendations\n",
        "- **Multiple Formats**: Exports in markdown and PDF formats\n",
        "- **Professional Quality**: Enterprise-ready reports and visualizations\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n",
            "Setting up analysis environment...\n",
            "Analysis started at: 2025-10-07 14:40:24\n",
            "Beginning Great Expectations validation analysis...\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import requests\n",
        "from dotenv import dotenv_values\n",
        "import warnings\n",
        "import time\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Start timing the entire analysis\n",
        "start_time = time.time()\n",
        "analysis_start = datetime.now()\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Setting up analysis environment...\")\n",
        "print(f\"Analysis started at: {analysis_start.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"Beginning Great Expectations validation analysis...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ollama URL: https://ollama.com\n",
            "Ollama Model: gpt-oss:20b\n",
            "API Key configured: Yes\n",
            "Validation path: ../../BirdiDQ/gx/uncommitted/validations\n",
            "Path exists: True\n",
            "Validation Analyzer initialized!\n"
          ]
        }
      ],
      "source": [
        "# Configuration and Setup\n",
        "class ValidationAnalyzer:\n",
        "    def __init__(self, validation_path=\"../../BirdiDQ/gx/uncommitted/validations\"):\n",
        "        self.validation_path = Path(validation_path)\n",
        "        self.results_data = []\n",
        "        self.analysis_results = {}\n",
        "        \n",
        "        # Ollama Configuration - Using your existing .env structure\n",
        "        env_path = Path(\"../../.env\")\n",
        "        if env_path.exists():\n",
        "            env_vars = dotenv_values(env_path)\n",
        "            # Use Ollama Cloud settings from your .env file\n",
        "            self.ollama_url = env_vars.get(\"OLLAMA_CLOUD_BASE_URL\", \"https://ollama.com\")\n",
        "            self.ollama_model = env_vars.get(\"OLLAMA_CLOUD_MODEL\", \"gpt-oss:20b\")\n",
        "            self.ollama_api_key = env_vars.get(\"OLLAMA_API_KEY\", \"\")\n",
        "            \n",
        "            # Fallback to local if cloud is not configured\n",
        "            if not self.ollama_api_key or self.ollama_api_key == \"your_api_key_here\":\n",
        "                print(\"No API key found, falling back to local Ollama\")\n",
        "                self.ollama_url = env_vars.get(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
        "                self.ollama_model = env_vars.get(\"OLLAMA_LOCAL_MODEL\", \"phi3:mini\")\n",
        "                self.ollama_api_key = \"\"\n",
        "        else:\n",
        "            # Default to local Ollama\n",
        "            self.ollama_url = \"http://localhost:11434\"\n",
        "            self.ollama_model = \"phi3:mini\"\n",
        "            self.ollama_api_key = \"\"\n",
        "        \n",
        "        print(f\"Ollama URL: {self.ollama_url}\")\n",
        "        print(f\"Ollama Model: {self.ollama_model}\")\n",
        "        print(f\"API Key configured: {'Yes' if self.ollama_api_key and self.ollama_api_key != 'your_api_key_here' else 'No'}\")\n",
        "        print(f\"Validation path: {self.validation_path}\")\n",
        "        print(f\"Path exists: {self.validation_path.exists()}\")\n",
        "        \n",
        "    def ollama_infer(self, prompt, model=None, url=None, timeout=120):\n",
        "        \"\"\"Send a prompt to Ollama and return the response\"\"\"\n",
        "        model = model or self.ollama_model\n",
        "        url = url or self.ollama_url\n",
        "        \n",
        "        # Prepare headers\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        \n",
        "        # Add API key if available (for Ollama Cloud)\n",
        "        if self.ollama_api_key and self.ollama_api_key != \"your_api_key_here\":\n",
        "            headers[\"Authorization\"] = f\"Bearer {self.ollama_api_key}\"\n",
        "        \n",
        "        try:\n",
        "            response = requests.post(\n",
        "                f\"{url}/api/generate\",\n",
        "                json={\"model\": model, \"prompt\": prompt, \"stream\": False},\n",
        "                headers=headers,\n",
        "                timeout=timeout\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            return response.json()[\"response\"]\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"Ollama request timed out after {timeout} seconds\")\n",
        "            return None\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error calling Ollama API: {e}\")\n",
        "            if hasattr(e, 'response') and e.response is not None:\n",
        "                print(f\"Response status: {e.response.status_code}\")\n",
        "                print(f\"Response text: {e.response.text}\")\n",
        "            return None\n",
        "\n",
        "# Initialize analyzer\n",
        "analyzer = ValidationAnalyzer()\n",
        "print(\"Validation Analyzer initialized!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading validation files...\n",
            "Loaded 1 validation files\n",
            "Processed 132 individual expectations\n",
            "Unique expectation suites: 1\n",
            "Unique expectation types: 15\n"
          ]
        }
      ],
      "source": [
        "# Data Loading and Processing Functions\n",
        "def load_validation_files(analyzer):\n",
        "    \"\"\"Load all validation JSON files from the directory structure\"\"\"\n",
        "    validation_files = []\n",
        "    \n",
        "    # Find all JSON files in the validation directory\n",
        "    for json_file in analyzer.validation_path.rglob(\"*.json\"):\n",
        "        try:\n",
        "            with open(json_file, 'r') as f:\n",
        "                data = json.load(f)\n",
        "                validation_files.append({\n",
        "                    'file_path': str(json_file),\n",
        "                    'data': data,\n",
        "                    'timestamp': data.get('meta', {}).get('validation_time', ''),\n",
        "                    'suite_name': data.get('meta', {}).get('expectation_suite_name', ''),\n",
        "                    'run_id': data.get('meta', {}).get('run_id', {}).get('run_name', ''),\n",
        "                    'data_asset': data.get('meta', {}).get('active_batch_definition', {}).get('data_asset_name', '')\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {json_file}: {e}\")\n",
        "    \n",
        "    analyzer.results_data = validation_files\n",
        "    print(f\"Loaded {len(validation_files)} validation files\")\n",
        "    return validation_files\n",
        "\n",
        "def process_validation_results(validation_files):\n",
        "    \"\"\"Process validation results into structured data\"\"\"\n",
        "    processed_data = []\n",
        "    \n",
        "    for file_info in validation_files:\n",
        "        data = file_info['data']\n",
        "        results = data.get('results', [])\n",
        "        \n",
        "        for result in results:\n",
        "            expectation_config = result.get('expectation_config', {})\n",
        "            exception_info = result.get('exception_info', {})\n",
        "            \n",
        "            processed_data.append({\n",
        "                'file_path': file_info['file_path'],\n",
        "                'timestamp': file_info['timestamp'],\n",
        "                'suite_name': file_info['suite_name'],\n",
        "                'run_id': file_info['run_id'],\n",
        "                'data_asset': file_info['data_asset'],\n",
        "                'expectation_type': expectation_config.get('expectation_type', ''),\n",
        "                'column': expectation_config.get('kwargs', {}).get('column', 'table-level'),\n",
        "                'success': result.get('success', False),\n",
        "                'exception_raised': exception_info.get('raised_exception', False),\n",
        "                'exception_message': exception_info.get('exception_message', ''),\n",
        "                'result': result.get('result', {}),\n",
        "                'meta': expectation_config.get('meta', {})\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(processed_data)\n",
        "\n",
        "# Load and process data\n",
        "print(\"Loading validation files...\")\n",
        "validation_files = load_validation_files(analyzer)\n",
        "df = process_validation_results(validation_files)\n",
        "\n",
        "print(f\"Processed {len(df)} individual expectations\")\n",
        "# print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
        "print(f\"Unique expectation suites: {df['suite_name'].nunique()}\")\n",
        "print(f\"Unique expectation types: {df['expectation_type'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating quality metrics...\n",
            "Overall Success Rate: 96.21%\n",
            "Exception Rate: 0.00%\n",
            "Total Expectations Analyzed: 132\n",
            "\n",
            " Suite Performance:\n",
            "                                      total_expectations  \\\n",
            "suite_name                                                 \n",
            "nyc_taxi_data_onboarding_suite_final                 132   \n",
            "\n",
            "                                      successful_expectations  success_rate  \\\n",
            "suite_name                                                                    \n",
            "nyc_taxi_data_onboarding_suite_final                      127         0.962   \n",
            "\n",
            "                                      exceptions  \n",
            "suite_name                                        \n",
            "nyc_taxi_data_onboarding_suite_final           0  \n",
            "\n",
            " Expectation Type Performance:\n",
            "                                                    total_expectations  \\\n",
            "expectation_type                                                         \n",
            "expect_column_max_to_be_between                                     14   \n",
            "expect_column_mean_to_be_between                                    12   \n",
            "expect_column_median_to_be_between                                  12   \n",
            "expect_column_min_to_be_between                                     14   \n",
            "expect_column_proportion_of_unique_values_to_be...                   8   \n",
            "\n",
            "                                                    successful_expectations  \\\n",
            "expectation_type                                                              \n",
            "expect_column_max_to_be_between                                          14   \n",
            "expect_column_mean_to_be_between                                          7   \n",
            "expect_column_median_to_be_between                                       12   \n",
            "expect_column_min_to_be_between                                          14   \n",
            "expect_column_proportion_of_unique_values_to_be...                        8   \n",
            "\n",
            "                                                    success_rate  exceptions  \n",
            "expectation_type                                                              \n",
            "expect_column_max_to_be_between                            1.000           0  \n",
            "expect_column_mean_to_be_between                           0.583           0  \n",
            "expect_column_median_to_be_between                         1.000           0  \n",
            "expect_column_min_to_be_between                            1.000           0  \n",
            "expect_column_proportion_of_unique_values_to_be...         1.000           0  \n"
          ]
        }
      ],
      "source": [
        "# Data Quality Metrics and Analysis\n",
        "def calculate_quality_metrics(df):\n",
        "    \"\"\"Calculate comprehensive data quality metrics\"\"\"\n",
        "    metrics = {}\n",
        "    \n",
        "    # Overall success rate\n",
        "    metrics['overall_success_rate'] = df['success'].mean()\n",
        "    \n",
        "    # Success rate by expectation suite\n",
        "    suite_metrics = df.groupby('suite_name').agg({\n",
        "        'success': ['count', 'sum', 'mean'],\n",
        "        'exception_raised': 'sum'\n",
        "    }).round(3)\n",
        "    suite_metrics.columns = ['total_expectations', 'successful_expectations', 'success_rate', 'exceptions']\n",
        "    metrics['suite_metrics'] = suite_metrics\n",
        "    \n",
        "    # Success rate by expectation type\n",
        "    type_metrics = df.groupby('expectation_type').agg({\n",
        "        'success': ['count', 'sum', 'mean'],\n",
        "        'exception_raised': 'sum'\n",
        "    }).round(3)\n",
        "    type_metrics.columns = ['total_expectations', 'successful_expectations', 'success_rate', 'exceptions']\n",
        "    metrics['type_metrics'] = type_metrics\n",
        "    \n",
        "    # Column-level analysis\n",
        "    column_metrics = df[df['column'] != 'table-level'].groupby('column').agg({\n",
        "        'success': ['count', 'sum', 'mean'],\n",
        "        'exception_raised': 'sum'\n",
        "    }).round(3)\n",
        "    column_metrics.columns = ['total_expectations', 'successful_expectations', 'success_rate', 'exceptions']\n",
        "    metrics['column_metrics'] = column_metrics\n",
        "    \n",
        "    # Exception analysis\n",
        "    exception_df = df[df['exception_raised'] == True]\n",
        "    metrics['exception_count'] = len(exception_df)\n",
        "    metrics['exception_rate'] = len(exception_df) / len(df)\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"Calculating quality metrics...\")\n",
        "quality_metrics = calculate_quality_metrics(df)\n",
        "\n",
        "print(f\"Overall Success Rate: {quality_metrics['overall_success_rate']:.2%}\")\n",
        "print(f\"Exception Rate: {quality_metrics['exception_rate']:.2%}\")\n",
        "print(f\"Total Expectations Analyzed: {len(df)}\")\n",
        "\n",
        "# Display top-level metrics\n",
        "print(\"\\n Suite Performance:\")\n",
        "print(quality_metrics['suite_metrics'].head())\n",
        "\n",
        "print(\"\\n Expectation Type Performance:\")\n",
        "print(quality_metrics['type_metrics'].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Generating AI insights with Ollama Cloud...\n",
            " AI Analysis Complete!\n",
            "\n",
            "================================================================================\n",
            " AI-POWERED DATA QUALITY INSIGHTS\n",
            "================================================================================\n",
            "# Great Expectations Data Quality Report  \n",
            "**Run ID:** `20251005T180117.592126Z`  \n",
            "**Date of report:** October 7 2025  \n",
            "\n",
            "> **Scope** – Validation run on the *nyc_taxi_data_onboarding_suite_final* suite (132 expectation checks) covering a single‑record snapshot of the NYC taxi dataset.  \n",
            "\n",
            "---\n",
            "\n",
            "## 1. Executive Summary  \n",
            "\n",
            "| Metric | Value |\n",
            "|--------|-------|\n",
            "| Total Expectations | 132 |\n",
            "| Successful Expectations | 127 |\n",
            "| Failure Rate | 3.79 % |\n",
            "| Success Rate | 96.21 % |\n",
            "| Exceptions | 0 |\n",
            "| Suite Count | 1 |\n",
            "\n",
            "The validation run demonstrates **overall good data quality**. 96 % of expectations pass and there are **no runtime exceptions**, indicating that the underlying schema and ingest process are functioning as expected.  \n",
            "\n",
            "The sole source of concern is **mean‑value expectations**:\n",
            "\n",
            "* 12 mean expectations were defined, of which only 7 passed (58 % success).  \n",
            "* All maximum‑value and median‑value expectations passed at 100 %.  \n",
            "\n",
            "The mean‑value failures suggest that **some columns have average values that lie outside the declared ranges** (e.g. `total_amount`, `trip_distance`, etc.).  These deviations do not represent a systemic data‑integrity problem (no schema violations, no missing columns), but they may affect downstream analytics that rely on statistically‑derived metrics.  \n",
            "\n",
            "---\n",
            "\n",
            "## 2. Critical Issues  \n",
            "\n",
            "| Issue | Why it matters | Impact |\n",
            "|-------|----------------|--------|\n",
            "| **Mean expectation failures (5 out of 12)** | Indicates that the statistical distribution of key columns is not within the acceptable bounds defined by business rules or previous historical averages. | Possible mis‑interpretation of fare amounts, trip metrics, or driver incentives; risk of incorrect business decisions or KPI reporting. |\n",
            "| **Single‑timestamp data snapshot** | The current run covers a single data point (2025‑10‑05 18:01:17).  No temporal trend analysis is possible from this snapshot alone, limiting validation depth. | Inability to detect seasonality, drift, or gradual degradation across epochs. |\n",
            "| **No exceptions reported** | While good for overall integrity, it may mask underlying data issues that do not trigger validation errors—for instance, unexpected nulls in hidden columns or foreign‑key mis‑references not covered by the suite. | Potential for silent data quality drift in future runs. |\n",
            "\n",
            "---\n",
            "\n",
            "## 3. Trends Analysis  \n",
            "\n",
            "| Trend | Observation | Notes |\n",
            "|-------|-------------|-------|\n",
            "| **Consistent high success for structural checks** | All column‑maximum, median, and basic existence checks passed. | The schema is stable and consistent with expectations. |\n",
            "| **Inconsistent statistical checks** | Mean checks have a 58 % pass rate whereas max/median checks pass at 100 %. | Likely due to outlier or drift in underlying data values, not necessarily a structural issue. |\n",
            "| **Temporal limitation** | Validation window is a single second; no historical comparison possible. | Future runs should span multiple days or weeks to surface performance trends. |\n",
            "| **Zero‑exception rate** | No test failures due to runtime exceptions. | Provides confidence that the validation harness is configured correctly. |\n",
            "\n",
            "---\n",
            "\n",
            "## 4. Recommendations  \n",
            "\n",
            "| Recommendation | Target | Action Steps | Owner |\n",
            "|----------------|--------|--------------|-------|\n",
            "| **Adjust mean expectation ranges** | Columns with failing mean checks | • Validate current bounds against recent historical data (last 7 days). <br>• Re‑calculate means for each column and update `expect_column_mean_to_be_between` thresholds accordingly. | Data Engineering |\n",
            "| **Implement dynamic expectation calibration** | All statistical expectations | • Add a pre‑validation step that calculates current statistics and automatically updates expectation ranges (e.g., using a percentile‑based approach). <br>• Schedule calibration weekly. | Data Ops |\n",
            "| **Expand benchmark windows** | Validation suite | • Extend the `batch_kwargs` to fetch a 7‑day window (or hourly buckets) rather than a single timestamp. <br>• Add expectation types that detect trend drift (`expect_column_mean_to_change_by`). | Analytics |\n",
            "| **Introduce anomaly detection** | Emerging patterns | • Add `expect_column_mean_to_be_increasing` or `expect_column_mean_to_be_decreasing` to detect unexpected reversals. | Data Quality Team |\n",
            "| **Schedule automated alerts** | Mean expectation failures | • Configure Slack or email alerts when any mean expectation fails. | Data Ops |\n",
            "| **Review data ingestion pipeline** | Data quality continuity | • Conduct a root‑cause analysis of the mean outliers: data source changes, transformations, or data source integrity. | ETL Developers |\n",
            "| **Document expectation rationale** | Governance | • Create documentation linking each expectation to a business requirement. This aids future stakeholders in understanding the purpose of bounds. | Knowledge Base |\n",
            "\n",
            "---\n",
            "\n",
            "## 5. Risk Assessment  \n",
            "\n",
            "| Risk | Likelihood | Impact | Mitigation |\n",
            "|------|------------|--------|------------|\n",
            "| **Statistical bias in downstream analytics** | Medium | High | Tighten mean expectations, periodic calibration. |\n",
            "| **Undetected data drift** | Low | Medium | Expand batch window and add drift‑detection expectations. |\n",
            "| **Operational disruption due to false positives** | Low | Low | Validate adjusted thresholds against stakeholders before deployment. |\n",
            "| **Increased maintenance overhead** | Medium | Medium | Automate expectation updates via CI/CD pipeline. |\n",
            "| **Loss of stakeholder trust** | Low | High | Communicate changes, provide evidence of improved accuracy. |\n",
            "\n",
            "---\n",
            "\n",
            "## 6. Next Steps (Prioritized Action Items)  \n",
            "\n",
            "| # | Action | Owner | Due Date |\n",
            "|---|--------|-------|----------|\n",
            "| 1 | **Root‑cause analysis of mean failures** | ETL Developers | 2025‑10‑15 |\n",
            "| 2 | **Re‑calculate and update mean thresholds** | Data Engineering | 2025‑10‑20 |\n",
            "| 3 | **Implement automated calibration of statistical expectations** | Data Ops | 2025‑10‑25 |\n",
            "| 4 | **Extend validation window to 7 days** | Analytics | 2025‑10‑28 |\n",
            "| 5 | **Add drift‑detection expectations** | Data Quality Team | 2025‑11‑02 |\n",
            "| 6 | **Deploy alerting mechanism for mean failures** | Data Ops | 2025‑11‑05 |\n",
            "| 7 | **Generate stakeholder briefing on updated expectations** | Knowledge Base | 2025‑11‑07 |\n",
            "\n",
            "---\n",
            "\n",
            "### Closing Remarks  \n",
            "\n",
            "The current validation shows that the **structural integrity of the dataset is intact**, but **statistical expectations on mean values require adjustment** to better reflect the observed data distribution. Proactively calibrating these expectations and expanding the temporal scope of validation will provide more robust, actionable insights and mitigate the risk of inaccurate reporting.\n"
          ]
        }
      ],
      "source": [
        "# AI-Powered Analysis with Ollama Cloud + Fallback\n",
        "def generate_fallback_analysis(df, quality_metrics):\n",
        "    \"\"\"Generate fallback analysis when AI is unavailable\"\"\"\n",
        "    \n",
        "    fallback_analysis = f\"\"\"\n",
        "## Executive Summary\n",
        "Based on the analysis of {len(df)} data quality expectations across {df['suite_name'].nunique()} validation suites, the overall data quality success rate is {quality_metrics['overall_success_rate']:.2%}.\n",
        "\n",
        "## Critical Issues\n",
        "- **Exception Rate**: {quality_metrics['exception_rate']:.2%} of expectations raised exceptions\n",
        "- **Lowest Performing Suite**: {quality_metrics['suite_metrics'].nsmallest(1, 'success_rate').index[0]} with {quality_metrics['suite_metrics'].nsmallest(1, 'success_rate')['success_rate'].iloc[0]:.2%} success rate\n",
        "- **Most Problematic Expectation Type**: {quality_metrics['type_metrics'].nsmallest(1, 'success_rate').index[0]} with {quality_metrics['type_metrics'].nsmallest(1, 'success_rate')['success_rate'].iloc[0]:.2%} success rate\n",
        "\n",
        "## Trends Analysis\n",
        "- **Date Range**: {df['timestamp'].min()} to {df['timestamp'].max()}\n",
        "- **Total Expectations**: {len(df)}\n",
        "- **Successful Expectations**: {df['success'].sum()}\n",
        "- **Failed Expectations**: {len(df) - df['success'].sum()}\n",
        "\n",
        "## Recommendations\n",
        "1. **Immediate Action**: Address suites with success rates below 80%\n",
        "2. **Expectation Review**: Review and update failing expectation types\n",
        "3. **Monitoring**: Implement daily monitoring for critical data assets\n",
        "4. **Process Improvement**: Establish data quality governance processes\n",
        "\n",
        "## Risk Assessment\n",
        "- **High Risk**: Suites with success rates below 70% require immediate attention\n",
        "- **Medium Risk**: Suites with success rates between 70-85% need monitoring\n",
        "- **Low Risk**: Suites with success rates above 85% are performing well\n",
        "\n",
        "## Next Steps\n",
        "1. Prioritize fixing the lowest performing suite\n",
        "2. Review expectation configurations for failing types\n",
        "3. Implement automated monitoring and alerting\n",
        "4. Schedule regular data quality reviews\n",
        "\"\"\"\n",
        "    \n",
        "    return fallback_analysis\n",
        "\n",
        "def generate_ai_insights(df, quality_metrics, analyzer):\n",
        "    \"\"\"Generate AI-powered insights using Ollama Cloud with fallback\"\"\"\n",
        "    \n",
        "    # Prepare data summary for AI analysis\n",
        "    data_summary = {\n",
        "        'total_expectations': len(df),\n",
        "        'overall_success_rate': quality_metrics['overall_success_rate'],\n",
        "        'exception_rate': quality_metrics['exception_rate'],\n",
        "        'suite_count': df['suite_name'].nunique(),\n",
        "        'expectation_types': df['expectation_type'].nunique(),\n",
        "        'date_range': f\"{df['timestamp'].min()} to {df['timestamp'].max()}\",\n",
        "        'top_failing_suites': quality_metrics['suite_metrics'].nsmallest(3, 'success_rate').to_dict(),\n",
        "        'top_failing_types': quality_metrics['type_metrics'].nsmallest(3, 'success_rate').to_dict()\n",
        "    }\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "    You are a data quality expert analyzing Great Expectations validation results. \n",
        "\n",
        "    Data Summary:\n",
        "    - Total Expectations: {data_summary['total_expectations']}\n",
        "    - Overall Success Rate: {data_summary['overall_success_rate']:.2%}\n",
        "    - Exception Rate: {data_summary['exception_rate']:.2%}\n",
        "    - Number of Suites: {data_summary['suite_count']}\n",
        "    - Number of Expectation Types: {data_summary['expectation_types']}\n",
        "    - Date Range: {data_summary['date_range']}\n",
        "\n",
        "    Top Failing Suites:\n",
        "    {data_summary['top_failing_suites']}\n",
        "\n",
        "    Top Failing Expectation Types:\n",
        "    {data_summary['top_failing_types']}\n",
        "\n",
        "    Please provide:\n",
        "    1. **Executive Summary**: Key findings and overall data quality assessment\n",
        "    2. **Critical Issues**: Most important problems that need immediate attention\n",
        "    3. **Trends Analysis**: Patterns and trends observed in the data\n",
        "    4. **Recommendations**: Specific actionable recommendations to improve data quality\n",
        "    5. **Risk Assessment**: Potential risks and their impact\n",
        "    6. **Next Steps**: Prioritized action items\n",
        "\n",
        "    Format your response as a professional data quality report with clear sections and actionable insights.\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\" Generating AI insights with Ollama Cloud...\")\n",
        "    ai_response = analyzer.ollama_infer(prompt)\n",
        "    \n",
        "    # Use fallback if AI is unavailable\n",
        "    if ai_response is None:\n",
        "        print(\" Ollama Cloud unavailable, using fallback analysis...\")\n",
        "        ai_response = generate_fallback_analysis(df, quality_metrics)\n",
        "    \n",
        "    return ai_response, data_summary\n",
        "\n",
        "# Generate AI insights\n",
        "ai_insights, data_summary = generate_ai_insights(df, quality_metrics, analyzer)\n",
        "\n",
        "print(\" AI Analysis Complete!\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" AI-POWERED DATA QUALITY INSIGHTS\")\n",
        "print(\"=\"*80)\n",
        "print(ai_insights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating comprehensive report...\n",
            "Report saved to: notebooks/great_expectations/validation_analysis_report.md\n",
            "Report generation complete!\n",
            "Report location: notebooks/great_expectations/validation_analysis_report.md\n"
          ]
        }
      ],
      "source": [
        "# Report Generation Functions\n",
        "def generate_markdown_report(df, quality_metrics, ai_insights, data_summary):\n",
        "    \"\"\"Generate comprehensive markdown report\"\"\"\n",
        "    \n",
        "    report = f\"\"\"# Great Expectations Validation Analysis Report\n",
        "\n",
        "**Generated on:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n",
        "**Analysis Period:** {data_summary['date_range']}\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "This report analyzes {data_summary['total_expectations']} data quality expectations across {data_summary['suite_count']} validation suites.\n",
        "\n",
        "### Key Metrics\n",
        "- **Overall Success Rate:** {data_summary['overall_success_rate']:.2%}\n",
        "- **Exception Rate:** {data_summary['exception_rate']:.2%}\n",
        "- **Expectation Types Analyzed:** {data_summary['expectation_types']}\n",
        "\n",
        "## Data Quality Metrics\n",
        "\n",
        "### Suite Performance\n",
        "\"\"\"\n",
        "    \n",
        "    # Add suite metrics table\n",
        "    suite_table = quality_metrics['suite_metrics'].to_markdown()\n",
        "    report += f\"\\n{suite_table}\\n\"\n",
        "    \n",
        "    report += f\"\"\"\n",
        "### Expectation Type Performance\n",
        "\"\"\"\n",
        "    \n",
        "    # Add type metrics table\n",
        "    type_table = quality_metrics['type_metrics'].to_markdown()\n",
        "    report += f\"\\n{type_table}\\n\"\n",
        "    \n",
        "    report += f\"\"\"\n",
        "## AI-Powered Analysis\n",
        "\n",
        "{ai_insights}\n",
        "\n",
        "## Detailed Analysis\n",
        "\n",
        "### Top Performing Suites\n",
        "    \"\"\"\n",
        "    \n",
        "    # Top performing suites\n",
        "    top_suites = quality_metrics['suite_metrics'].nlargest(5, 'success_rate')\n",
        "    for suite, metrics in top_suites.iterrows():\n",
        "        report += f\"- **{suite}**: {metrics['success_rate']:.2%} success rate ({metrics['successful_expectations']}/{metrics['total_expectations']} expectations)\\n\"\n",
        "    \n",
        "    report += f\"\"\"\n",
        "    ### Areas Requiring Attention\n",
        "    \"\"\"\n",
        "    \n",
        "    # Bottom performing suites\n",
        "    bottom_suites = quality_metrics['suite_metrics'].nsmallest(5, 'success_rate')\n",
        "    for suite, metrics in bottom_suites.iterrows():\n",
        "        report += f\"- **{suite}**: {metrics['success_rate']:.2%} success rate ({metrics['successful_expectations']}/{metrics['total_expectations']} expectations)\\n\"\n",
        "    \n",
        "    report += f\"\"\"\n",
        "    ## Recommendations\n",
        "\n",
        "    Based on the analysis, the following actions are recommended:\n",
        "\n",
        "    1. **Immediate Actions**: Address suites with success rates below 80%\n",
        "    2. **Monitoring**: Implement daily monitoring for critical data assets\n",
        "    3. **Expectation Review**: Review and update failing expectation types\n",
        "    4. **Process Improvement**: Establish data quality governance processes\n",
        "\n",
        "    ## Technical Details\n",
        "\n",
        "    - **Analysis Engine**: Great Expectations v0.18.22\n",
        "    - **AI Analysis**: Ollama LLM (gpt-oss:20b)\n",
        "    - **Data Source**: Validation results from BirdiDQ/gx/uncommitted/validations\n",
        "    - **Report Generated**: {datetime.now().isoformat()}\n",
        "\n",
        "    ---\n",
        "    *This report was automatically generated by the Great Expectations Validation Analysis system.*\n",
        "    \"\"\"\n",
        "    \n",
        "    return report\n",
        "\n",
        "def save_report(report, filename=\"validation_analysis_report.md\"):\n",
        "    \"\"\"Save report to markdown file\"\"\"\n",
        "    output_path = Path(\"notebooks/great_expectations\") / filename\n",
        "    \n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(report)\n",
        "    \n",
        "    print(f\"Report saved to: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "# Generate and save report\n",
        "print(\"Generating comprehensive report...\")\n",
        "markdown_report = generate_markdown_report(df, quality_metrics, ai_insights, data_summary)\n",
        "report_path = save_report(markdown_report)\n",
        "\n",
        "print(\"Report generation complete!\")\n",
        "print(f\"Report location: {report_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data catalog already generated!\n",
            "Data assets cataloged: 1\n",
            "Expectation suites cataloged: 1\n"
          ]
        }
      ],
      "source": [
        "# Data Catalog Generation Functions\n",
        "def generate_data_catalog(df, quality_metrics, validation_files):\n",
        "    \"\"\"Generate comprehensive data catalog from validation results\"\"\"\n",
        "    \n",
        "    catalog = {\n",
        "        \"metadata\": {\n",
        "            \"generated_on\": datetime.now().isoformat(),\n",
        "            \"total_validation_files\": len(validation_files),\n",
        "            \"analysis_period\": f\"{df['timestamp'].min()} to {df['timestamp'].max()}\",\n",
        "            \"great_expectations_version\": \"0.18.22\"\n",
        "        },\n",
        "        \"data_assets\": {},\n",
        "        \"expectation_suites\": {},\n",
        "        \"data_quality_summary\": {\n",
        "            \"overall_success_rate\": quality_metrics['overall_success_rate'],\n",
        "            \"exception_rate\": quality_metrics['exception_rate'],\n",
        "            \"total_expectations\": len(df)\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Process each validation file to extract data asset information\n",
        "    for file_info in validation_files:\n",
        "        data = file_info['data']\n",
        "        suite_name = file_info['suite_name']\n",
        "        data_asset = file_info['data_asset']\n",
        "        \n",
        "        # Extract batch definition information\n",
        "        batch_def = data.get('meta', {}).get('active_batch_definition', {})\n",
        "        batch_spec = data.get('meta', {}).get('batch_spec', {})\n",
        "        \n",
        "        # Initialize data asset entry if not exists\n",
        "        if data_asset not in catalog[\"data_assets\"]:\n",
        "            catalog[\"data_assets\"][data_asset] = {\n",
        "                \"name\": data_asset,\n",
        "                \"type\": batch_spec.get('type', 'unknown'),\n",
        "                \"table_name\": batch_spec.get('table_name', ''),\n",
        "                \"schema_name\": batch_spec.get('schema_name', ''),\n",
        "                \"datasource\": batch_def.get('datasource_name', ''),\n",
        "                \"data_connector\": batch_def.get('data_connector_name', ''),\n",
        "                \"validation_runs\": [],\n",
        "                \"columns\": {},\n",
        "                \"expectation_suites\": []\n",
        "            }\n",
        "        \n",
        "        # Add validation run information\n",
        "        run_info = {\n",
        "            \"run_id\": file_info['run_id'],\n",
        "            \"timestamp\": file_info['timestamp'],\n",
        "            \"suite_name\": suite_name,\n",
        "            \"expectation_count\": len(data.get('results', [])),\n",
        "            \"success_rate\": sum(1 for r in data.get('results', []) if r.get('success', False)) / len(data.get('results', [])) if data.get('results') else 0\n",
        "        }\n",
        "        \n",
        "        catalog[\"data_assets\"][data_asset][\"validation_runs\"].append(run_info)\n",
        "        \n",
        "        # Add suite to data asset\n",
        "        if suite_name not in catalog[\"data_assets\"][data_asset][\"expectation_suites\"]:\n",
        "            catalog[\"data_assets\"][data_asset][\"expectation_suites\"].append(suite_name)\n",
        "        \n",
        "        # Extract column information from expectations\n",
        "        for result in data.get('results', []):\n",
        "            expectation_config = result.get('expectation_config', {})\n",
        "            column = expectation_config.get('kwargs', {}).get('column', 'table-level')\n",
        "            \n",
        "            if column != 'table-level' and column not in catalog[\"data_assets\"][data_asset][\"columns\"]:\n",
        "                catalog[\"data_assets\"][data_asset][\"columns\"][column] = {\n",
        "                    \"name\": column,\n",
        "                    \"expectation_types\": [],\n",
        "                    \"quality_metrics\": {\n",
        "                        \"total_expectations\": 0,\n",
        "                        \"successful_expectations\": 0,\n",
        "                        \"success_rate\": 0.0,\n",
        "                        \"exceptions\": 0\n",
        "                    }\n",
        "                }\n",
        "            \n",
        "            if column != 'table-level':\n",
        "                exp_type = expectation_config.get('expectation_type', '')\n",
        "                if exp_type not in catalog[\"data_assets\"][data_asset][\"columns\"][column][\"expectation_types\"]:\n",
        "                    catalog[\"data_assets\"][data_asset][\"columns\"][column][\"expectation_types\"].append(exp_type)\n",
        "        \n",
        "        # Initialize expectation suite entry\n",
        "        if suite_name not in catalog[\"expectation_suites\"]:\n",
        "            catalog[\"expectation_suites\"][suite_name] = {\n",
        "                \"name\": suite_name,\n",
        "                \"data_assets\": [],\n",
        "                \"expectation_types\": [],\n",
        "                \"quality_metrics\": {\n",
        "                    \"total_expectations\": 0,\n",
        "                    \"successful_expectations\": 0,\n",
        "                    \"success_rate\": 0.0,\n",
        "                    \"exceptions\": 0\n",
        "                }\n",
        "            }\n",
        "        \n",
        "        # Add data asset to suite\n",
        "        if data_asset not in catalog[\"expectation_suites\"][suite_name][\"data_assets\"]:\n",
        "            catalog[\"expectation_suites\"][suite_name][\"data_assets\"].append(data_asset)\n",
        "    \n",
        "    # Calculate quality metrics for columns and suites\n",
        "    for data_asset_name, asset_info in catalog[\"data_assets\"].items():\n",
        "        for column_name, column_info in asset_info[\"columns\"].items():\n",
        "            column_df = df[(df['data_asset'] == data_asset_name) & (df['column'] == column_name)]\n",
        "            if not column_df.empty:\n",
        "                column_info[\"quality_metrics\"] = {\n",
        "                    \"total_expectations\": len(column_df),\n",
        "                    \"successful_expectations\": column_df['success'].sum(),\n",
        "                    \"success_rate\": column_df['success'].mean(),\n",
        "                    \"exceptions\": column_df['exception_raised'].sum()\n",
        "                }\n",
        "    \n",
        "    for suite_name, suite_info in catalog[\"expectation_suites\"].items():\n",
        "        suite_df = df[df['suite_name'] == suite_name]\n",
        "        if not suite_df.empty:\n",
        "            suite_info[\"quality_metrics\"] = {\n",
        "                \"total_expectations\": len(suite_df),\n",
        "                \"successful_expectations\": suite_df['success'].sum(),\n",
        "                \"success_rate\": suite_df['success'].mean(),\n",
        "                \"exceptions\": suite_df['exception_raised'].sum()\n",
        "            }\n",
        "            suite_info[\"expectation_types\"] = suite_df['expectation_type'].unique().tolist()\n",
        "    \n",
        "    return catalog\n",
        "\n",
        "def generate_data_catalog_report(catalog):\n",
        "    \"\"\"Generate markdown report for data catalog\"\"\"\n",
        "    \n",
        "    report = f\"\"\"# Data Catalog\n",
        "\n",
        "**Generated on:** {catalog['metadata']['generated_on']}  \n",
        "**Analysis Period:** {catalog['metadata']['analysis_period']}  \n",
        "**Total Validation Files:** {catalog['metadata']['total_validation_files']}\n",
        "\n",
        "## Data Quality Summary\n",
        "\n",
        "<div class=\"summary-box\">\n",
        "- **Overall Success Rate:** {catalog['data_quality_summary']['overall_success_rate']:.2%}\n",
        "- **Exception Rate:** {catalog['data_quality_summary']['exception_rate']:.2%}\n",
        "- **Total Expectations:** {catalog['data_quality_summary']['total_expectations']}\n",
        "\n",
        "\n",
        "## Data Assets\n",
        "\n",
        "\"\"\"\n",
        "    \n",
        "    for asset_name, asset_info in catalog[\"data_assets\"].items():\n",
        "        report += f\"\"\"### {asset_name}\n",
        "\n",
        "**Asset Type:** {asset_info['type']}  \n",
        "**Table:** {asset_info['table_name']}  \n",
        "**Schema:** {asset_info['schema_name']}  \n",
        "**Datasource:** {asset_info['datasource']}  \n",
        "**Data Connector:** {asset_info['data_connector']}\n",
        "\n",
        "**Expectation Suites:** {', '.join(asset_info['expectation_suites'])}  \n",
        "**Validation Runs:** {len(asset_info['validation_runs'])}\n",
        "\n",
        "#### Columns ({len(asset_info['columns'])})\n",
        "\n",
        "| Column Name | Expectations | Success Rate | Exceptions |\n",
        "|-------------|-------------|--------------|------------|\n",
        "\"\"\"\n",
        "        \n",
        "        for col_name, col_info in asset_info[\"columns\"].items():\n",
        "            metrics = col_info[\"quality_metrics\"]\n",
        "            report += f\"| {col_name} | {metrics['total_expectations']} | {metrics['success_rate']:.2%} | {metrics['exceptions']} |\\n\"\n",
        "        \n",
        "        report += \"\\n\"\n",
        "    \n",
        "    report += \"\"\"## Expectation Suites\n",
        "\n",
        "\"\"\"\n",
        "    \n",
        "    for suite_name, suite_info in catalog[\"expectation_suites\"].items():\n",
        "        metrics = suite_info[\"quality_metrics\"]\n",
        "        report += f\"\"\"### {suite_name}\n",
        "\n",
        "**Data Assets:** {', '.join(suite_info['data_assets'])}  \n",
        "**Total Expectations:** {metrics['total_expectations']}  \n",
        "**Success Rate:** {metrics['success_rate']:.2%}  \n",
        "**Exceptions:** {metrics['exceptions']}\n",
        "\n",
        "**Expectation Types:** {', '.join(suite_info['expectation_types'])}\n",
        "\n",
        "\"\"\"\n",
        "    \n",
        "    report += \"\"\"## Validation Run History\n",
        "\n",
        "<table class=\"appendix-table\">\n",
        "<thead>\n",
        "<tr>\n",
        "<th>Data Asset</th>\n",
        "<th>Suite</th>\n",
        "<th>Run ID</th>\n",
        "<th>Timestamp</th>\n",
        "<th>Expectations</th>\n",
        "<th>Success Rate</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "\"\"\"\n",
        "    \n",
        "    for asset_name, asset_info in catalog[\"data_assets\"].items():\n",
        "        for run in asset_info[\"validation_runs\"]:\n",
        "            report += f\"\"\"<tr>\n",
        "<td>{asset_name}</td>\n",
        "<td>{run['suite_name']}</td>\n",
        "<td>{run['run_id']}</td>\n",
        "<td>{run['timestamp']}</td>\n",
        "<td>{run['expectation_count']}</td>\n",
        "<td>{run['success_rate']:.2%}</td>\n",
        "</tr>\n",
        "\"\"\"\n",
        "    \n",
        "    report += \"\"\"</tbody>\n",
        "</table>\n",
        "\n",
        "---\n",
        "*This data catalog was automatically generated by the Great Expectations Validation Analysis system.*\n",
        "\"\"\"\n",
        "    \n",
        "    return report\n",
        "\n",
        "# Data Catalog Generation (Run this cell if data_catalog is not already generated)\n",
        "if 'data_catalog' not in locals():\n",
        "    print(\"Generating data catalog...\")\n",
        "    data_catalog = generate_data_catalog(df, quality_metrics, validation_files)\n",
        "    \n",
        "    # Save data catalog as JSON\n",
        "    catalog_json_path = Path(\"notebooks/great_expectations\") / \"data_catalog.json\"\n",
        "    with open(catalog_json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data_catalog, f, indent=2, default=str)\n",
        "    \n",
        "    print(f\"Data catalog JSON saved to: {catalog_json_path}\")\n",
        "    \n",
        "    # Generate data catalog report\n",
        "    catalog_report = generate_data_catalog_report(data_catalog)\n",
        "    \n",
        "    # Save data catalog report\n",
        "    catalog_report_path = Path(\"notebooks/great_expectations\") / \"data_catalog_report.md\"\n",
        "    with open(catalog_report_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(catalog_report)\n",
        "    \n",
        "    print(f\"Data catalog report saved to: {catalog_report_path}\")\n",
        "    \n",
        "    print(\"Data catalog generation complete!\")\n",
        "    print(f\"Data assets cataloged: {len(data_catalog['data_assets'])}\")\n",
        "    print(f\"Expectation suites cataloged: {len(data_catalog['expectation_suites'])}\")\n",
        "else:\n",
        "    print(\"Data catalog already generated!\")\n",
        "    print(f\"Data assets cataloged: {len(data_catalog['data_assets'])}\")\n",
        "    print(f\"Expectation suites cataloged: {len(data_catalog['expectation_suites'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating enhanced report with A4 formatting and data catalog...\n",
            "Enhanced markdown report saved to: notebooks/great_expectations/validation_analysis_report_enhanced.md\n",
            "Generating enhanced PDF report...\n",
            "📄 PDF report saved to: validation_analysis_report_enhanced.pdf\n",
            "Enhanced PDF report generated successfully!\n",
            "Enhanced PDF location: validation_analysis_report_enhanced.pdf\n"
          ]
        }
      ],
      "source": [
        "# Enhanced PDF Export with A4 Formatting and Appendix\n",
        "def generate_pdf_report(report_content, filename=\"validation_analysis_report.pdf\"):\n",
        "    \"\"\"Generate PDF report from markdown content with proper A4 formatting\"\"\"\n",
        "    try:\n",
        "        from markdown import markdown\n",
        "        from weasyprint import HTML, CSS\n",
        "        from weasyprint.text.fonts import FontConfiguration\n",
        "        \n",
        "        # Convert markdown to HTML\n",
        "        html_content = markdown(report_content, extensions=['tables', 'codehilite'])\n",
        "        \n",
        "        # Enhanced CSS styling for A4 format with proper margins and table handling\n",
        "        css_content = \"\"\"\n",
        "        @page {\n",
        "            size: A4;\n",
        "            margin: 2cm;\n",
        "            @top-center {\n",
        "                content: \"Great Expectations Validation Analysis Report\";\n",
        "                font-size: 10px;\n",
        "                color: #666;\n",
        "            }\n",
        "            @bottom-center {\n",
        "                content: \"Page \" counter(page) \" of \" counter(pages);\n",
        "                font-size: 10px;\n",
        "                color: #666;\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        body {\n",
        "            font-family: 'Arial', sans-serif;\n",
        "            line-height: 1.6;\n",
        "            margin: 0;\n",
        "            padding: 0;\n",
        "            color: #333;\n",
        "            font-size: 11px;\n",
        "        }\n",
        "        \n",
        "        h1 {\n",
        "            color: #2c3e50;\n",
        "            border-bottom: 3px solid #3498db;\n",
        "            padding-bottom: 10px;\n",
        "            page-break-after: avoid;\n",
        "            font-size: 18px;\n",
        "        }\n",
        "        \n",
        "        h2 {\n",
        "            color: #34495e;\n",
        "            margin-top: 25px;\n",
        "            border-bottom: 1px solid #bdc3c7;\n",
        "            padding-bottom: 5px;\n",
        "            page-break-after: avoid;\n",
        "            font-size: 14px;\n",
        "        }\n",
        "        \n",
        "        h3 {\n",
        "            color: #7f8c8d;\n",
        "            margin-top: 20px;\n",
        "            page-break-after: avoid;\n",
        "            font-size: 12px;\n",
        "        }\n",
        "        \n",
        "        /* Table styling for main content - compact */\n",
        "        table {\n",
        "            border-collapse: collapse;\n",
        "            width: 100%;\n",
        "            margin: 15px 0;\n",
        "            font-size: 9px;\n",
        "            page-break-inside: avoid;\n",
        "        }\n",
        "        \n",
        "        th, td {\n",
        "            border: 1px solid #ddd;\n",
        "            padding: 4px 6px;\n",
        "            text-align: left;\n",
        "            word-wrap: break-word;\n",
        "        }\n",
        "        \n",
        "        th {\n",
        "            background-color: #f2f2f2;\n",
        "            font-weight: bold;\n",
        "            font-size: 9px;\n",
        "        }\n",
        "        \n",
        "        /* Wide tables go to appendix */\n",
        "        .appendix-table {\n",
        "            font-size: 8px;\n",
        "            margin: 10px 0;\n",
        "        }\n",
        "        \n",
        "        .appendix-table th,\n",
        "        .appendix-table td {\n",
        "            padding: 2px 4px;\n",
        "            font-size: 8px;\n",
        "        }\n",
        "        \n",
        "        code {\n",
        "            background-color: #f4f4f4;\n",
        "            padding: 2px 4px;\n",
        "            border-radius: 3px;\n",
        "            font-family: 'Courier New', monospace;\n",
        "            font-size: 9px;\n",
        "        }\n",
        "        \n",
        "        pre {\n",
        "            background-color: #f4f4f4;\n",
        "            padding: 10px;\n",
        "            border-radius: 5px;\n",
        "            overflow-x: auto;\n",
        "            font-size: 9px;\n",
        "            page-break-inside: avoid;\n",
        "        }\n",
        "        \n",
        "        /* Page breaks */\n",
        "        .page-break {\n",
        "            page-break-before: always;\n",
        "        }\n",
        "        \n",
        "        /* Summary boxes */\n",
        "        .summary-box {\n",
        "            background-color: #f8f9fa;\n",
        "            border: 1px solid #dee2e6;\n",
        "            border-radius: 5px;\n",
        "            padding: 15px;\n",
        "            margin: 15px 0;\n",
        "        }\n",
        "        \n",
        "        /* Appendix styling */\n",
        "        .appendix {\n",
        "            page-break-before: always;\n",
        "        }\n",
        "        \n",
        "        .appendix h2 {\n",
        "            color: #e74c3c;\n",
        "            border-bottom: 2px solid #e74c3c;\n",
        "        }\n",
        "        \"\"\"\n",
        "        \n",
        "        # Create HTML document with proper structure\n",
        "        full_html = f\"\"\"\n",
        "        <!DOCTYPE html>\n",
        "        <html>\n",
        "        <head>\n",
        "            <meta charset=\"utf-8\">\n",
        "            <title>Great Expectations Validation Analysis Report</title>\n",
        "            <style>{css_content}</style>\n",
        "        </head>\n",
        "        <body>\n",
        "            {html_content}\n",
        "        </body>\n",
        "        </html>\n",
        "        \"\"\"\n",
        "        \n",
        "        # Generate PDF\n",
        "        output_path = Path(\".\").parent / filename\n",
        "        HTML(string=full_html).write_pdf(str(output_path))\n",
        "        \n",
        "        print(f\"📄 PDF report saved to: {output_path}\")\n",
        "        return output_path\n",
        "        \n",
        "    except ImportError as e:\n",
        "        print(f\"PDF generation requires additional packages: {e}\")\n",
        "        print(\"Install with: pip install weasyprint markdown\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_enhanced_markdown_report(df, quality_metrics, ai_insights, data_summary, data_catalog=None):\n",
        "    \"\"\"Generate enhanced markdown report with proper structure for A4 PDF\"\"\"\n",
        "    \n",
        "    # Extract key metrics for summary\n",
        "    overall_success = quality_metrics['overall_success_rate']\n",
        "    exception_rate = quality_metrics['exception_rate']\n",
        "    total_expectations = len(df)\n",
        "    \n",
        "    # Get top failing expectation types for summary\n",
        "    failing_types = quality_metrics['type_metrics'].nsmallest(3, 'success_rate')\n",
        "    \n",
        "    report = f\"\"\"# Great Expectations Validation Analysis Report\n",
        "\n",
        "**Generated on:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n",
        "**Analysis Period:** {data_summary['date_range']}\n",
        "\n",
        "\n",
        "### Executive Summary\n",
        "\n",
        "<div class=\"summary-box\">\n",
        "<h3>Executive Summary</h3>\n",
        "<p>This report analyzes <strong>{total_expectations}</strong> data quality expectations across <strong>{data_summary['suite_count']}</strong> validation suites.</p>\n",
        "\n",
        "<h4>Key Metrics</h4>\n",
        "<ul>\n",
        "<li><strong>Overall Success Rate:</strong> {overall_success:.2%}</li>\n",
        "<li><strong>Exception Rate:</strong> {exception_rate:.2%}</li>\n",
        "<li><strong>Expectation Types Analyzed:</strong> {data_summary['expectation_types']}</li>\n",
        "<li><strong>Critical Issues:</strong> {len(failing_types[failing_types['success_rate'] < 0.8])} expectation types below 80% success rate</li>\n",
        "</ul>\n",
        "</div>\n",
        "\n",
        "\n",
        "## Critical Findings\n",
        "\n",
        "### Top Issues Requiring Attention\n",
        "\"\"\"\n",
        "    \n",
        "    # Add critical issues in a compact format\n",
        "    for idx, (exp_type, metrics) in enumerate(failing_types.iterrows(), 1):\n",
        "        if metrics['success_rate'] < 0.8:\n",
        "            report += f\"{idx}. **{exp_type}**: {metrics['success_rate']:.1%} success rate ({metrics['successful_expectations']}/{metrics['total_expectations']} expectations)\\n\"\n",
        "    \n",
        "    report += f\"\"\"\n",
        "\n",
        "## AI-Powered Analysis\n",
        "\n",
        "{ai_insights}\n",
        "\n",
        "## Data Catalog Summary\n",
        "\n",
        "<div class=\"summary-box\">\n",
        "\"\"\"\n",
        "    \n",
        "    # Add data catalog summary if available\n",
        "    if data_catalog:\n",
        "        report += f\"\"\"**Data Assets:** {len(data_catalog['data_assets'])}  \n",
        "**Expectation Suites:** {len(data_catalog['expectation_suites'])}  \n",
        "**Validation Runs:** {sum(len(asset['validation_runs']) for asset in data_catalog['data_assets'].values())}  \n",
        "**Total Columns Monitored:** {sum(len(asset['columns']) for asset in data_catalog['data_assets'].values())}\n",
        "\"\"\"\n",
        "    else:\n",
        "        report += \"Data catalog not available\"\n",
        "    \n",
        "    report += \"\"\"\n",
        "\n",
        "## Recommendations\n",
        "\n",
        "Based on the analysis, the following actions are recommended:\n",
        "\n",
        "1. **Immediate Actions**: Address expectation types with success rates below 80%\n",
        "2. **Monitoring**: Implement daily monitoring for critical data assets  \n",
        "3. **Expectation Review**: Review and update failing expectation types\n",
        "4. **Process Improvement**: Establish data quality governance processes\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "- **Analysis Engine**: Great Expectations v0.18.22\n",
        "- **AI Analysis**: Ollama LLM (gpt-oss:20b)\n",
        "- **Data Source**: Validation results from BirdiDQ/gx/uncommitted/validations\n",
        "- **Report Generated**: {datetime.now().isoformat()}\n",
        "\n",
        "---\n",
        "\n",
        "<div class=\"page-break\">\n",
        "<div class=\"appendix\">\n",
        "\n",
        "## Appendix A: Detailed Suite Performance\n",
        "\n",
        "| Suite Name | Total Expectations | Successful | Success Rate | Exceptions |\n",
        "|------------|------------------|------------|--------------|------------|\n",
        "\"\"\"\n",
        "    \n",
        "    # Add suite metrics in compact format\n",
        "    for suite, metrics in quality_metrics['suite_metrics'].iterrows():\n",
        "        report += f\"| {suite} | {metrics['total_expectations']} | {metrics['successful_expectations']} | {metrics['success_rate']:.2%} | {metrics['exceptions']} |\\n\"\n",
        "    \n",
        "    report += f\"\"\"\n",
        "\n",
        "## Appendix B: Detailed Expectation Type Performance\n",
        "\n",
        "<table class=\"appendix-table\">\n",
        "<thead>\n",
        "<tr>\n",
        "<th>Expectation Type</th>\n",
        "<th>Total</th>\n",
        "<th>Successful</th>\n",
        "<th>Success Rate</th>\n",
        "<th>Exceptions</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "\"\"\"\n",
        "    \n",
        "    # Add expectation type metrics in compact format\n",
        "    for exp_type, metrics in quality_metrics['type_metrics'].iterrows():\n",
        "        report += f\"\"\"<tr>\n",
        "<td>{exp_type}</td>\n",
        "<td>{metrics['total_expectations']}</td>\n",
        "<td>{metrics['successful_expectations']}</td>\n",
        "<td>{metrics['success_rate']:.2%}</td>\n",
        "<td>{metrics['exceptions']}</td>\n",
        "</tr>\n",
        "\"\"\"\n",
        "    \n",
        "    report += \"\"\"</tbody>\n",
        "</table>\n",
        "\n",
        "\"\"\"\n",
        "    \n",
        "    # Add data catalog appendix if available\n",
        "    if data_catalog:\n",
        "        report += f\"\"\"## Appendix C: Data Catalog\n",
        "\n",
        "### Data Assets Overview\n",
        "\n",
        "<table class=\"appendix-table\">\n",
        "<thead>\n",
        "<tr>\n",
        "<th>Data Asset</th>\n",
        "<th>Type</th>\n",
        "<th>Table</th>\n",
        "<th>Schema</th>\n",
        "<th>Datasource</th>\n",
        "<th>Columns</th>\n",
        "<th>Suites</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "\"\"\"\n",
        "        \n",
        "        for asset_name, asset_info in data_catalog[\"data_assets\"].items():\n",
        "            report += f\"\"\"<tr>\n",
        "<td>{asset_name}</td>\n",
        "<td>{asset_info['type']}</td>\n",
        "<td>{asset_info['table_name']}</td>\n",
        "<td>{asset_info['schema_name']}</td>\n",
        "<td>{asset_info['datasource']}</td>\n",
        "<td>{len(asset_info['columns'])}</td>\n",
        "<td>{len(asset_info['expectation_suites'])}</td>\n",
        "</tr>\n",
        "\"\"\"\n",
        "        \n",
        "        report += \"\"\"</tbody>\n",
        "</table>\n",
        "\n",
        "### Column Quality Summary\n",
        "\n",
        "<table class=\"appendix-table\">\n",
        "<thead>\n",
        "<tr>\n",
        "<th>Data Asset</th>\n",
        "<th>Column</th>\n",
        "<th>Expectations</th>\n",
        "<th>Success Rate</th>\n",
        "<th>Exceptions</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "\"\"\"\n",
        "        \n",
        "        for asset_name, asset_info in data_catalog[\"data_assets\"].items():\n",
        "            for col_name, col_info in asset_info[\"columns\"].items():\n",
        "                metrics = col_info[\"quality_metrics\"]\n",
        "                report += f\"\"\"<tr>\n",
        "<td>{asset_name}</td>\n",
        "<td>{col_name}</td>\n",
        "<td>{metrics['total_expectations']}</td>\n",
        "<td>{metrics['success_rate']:.2%}</td>\n",
        "<td>{metrics['exceptions']}</td>\n",
        "</tr>\n",
        "\"\"\"\n",
        "        \n",
        "        report += \"\"\"</tbody>\n",
        "</table>\n",
        "\n",
        "\"\"\"\n",
        "    \n",
        "    report += \"\"\"\n",
        "\n",
        "---\n",
        "*This report was automatically generated by the Great Expectations Validation Analysis system.*\n",
        "\"\"\"\n",
        "    \n",
        "    return report\n",
        "\n",
        "# Generate enhanced report with data catalog\n",
        "print(\"Generating enhanced report with A4 formatting and data catalog...\")\n",
        "\n",
        "# First generate the data catalog if it doesn't exist\n",
        "if 'data_catalog' not in locals():\n",
        "    print(\"Generating data catalog first...\")\n",
        "    data_catalog = generate_data_catalog(df, quality_metrics, validation_files)\n",
        "    \n",
        "    # Save data catalog as JSON\n",
        "    catalog_json_path = Path(\"notebooks/great_expectations\") / \"data_catalog.json\"\n",
        "    with open(catalog_json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data_catalog, f, indent=2, default=str)\n",
        "    \n",
        "    print(f\"Data catalog JSON saved to: {catalog_json_path}\")\n",
        "\n",
        "enhanced_report = generate_enhanced_markdown_report(df, quality_metrics, ai_insights, data_summary, data_catalog)\n",
        "\n",
        "# Save enhanced markdown report\n",
        "enhanced_report_path = Path(\"notebooks/great_expectations\") / \"validation_analysis_report_enhanced.md\"\n",
        "with open(enhanced_report_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(enhanced_report)\n",
        "\n",
        "print(f\"Enhanced markdown report saved to: {enhanced_report_path}\")\n",
        "\n",
        "# Generate enhanced PDF report\n",
        "print(\"Generating enhanced PDF report...\")\n",
        "enhanced_pdf_path = generate_pdf_report(enhanced_report, \"validation_analysis_report_enhanced.pdf\")\n",
        "\n",
        "if enhanced_pdf_path:\n",
        "    print(f\"Enhanced PDF report generated successfully!\")\n",
        "    print(f\"Enhanced PDF location: {enhanced_pdf_path}\")\n",
        "else:\n",
        "    print(\"Enhanced PDF generation failed - check error messages above\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Mermaid Diagram Generation with Ollama AI\n",
        "# import base64\n",
        "# import io\n",
        "# from IPython.display import HTML, display\n",
        "# import json\n",
        "\n",
        "# def generate_mermaid_architecture(data_catalog, analyzer):\n",
        "#     \"\"\"Generate high-quality Mermaid architecture diagram using Ollama AI\"\"\"\n",
        "    \n",
        "#     # Prepare detailed context for AI\n",
        "#     architecture_context = {\n",
        "#         'data_assets': list(data_catalog['data_assets'].keys()),\n",
        "#         'datasources': list(set(asset['datasource'] for asset in data_catalog['data_assets'].values() if asset['datasource'])),\n",
        "#         'schemas': list(set(asset['schema_name'] for asset in data_catalog['data_assets'].values() if asset['schema_name'])),\n",
        "#         'expectation_suites': list(data_catalog['expectation_suites'].keys()),\n",
        "#         'total_columns': sum(len(asset['columns']) for asset in data_catalog['data_assets'].values()),\n",
        "#         'validation_runs': sum(len(asset['validation_runs']) for asset in data_catalog['data_assets'].values()),\n",
        "#         'asset_details': {name: {\n",
        "#             'type': info['type'],\n",
        "#             'table': info['table_name'],\n",
        "#             'schema': info['schema_name'],\n",
        "#             'datasource': info['datasource'],\n",
        "#             'columns': len(info['columns']),\n",
        "#             'suites': info['expectation_suites']\n",
        "#         } for name, info in data_catalog['data_assets'].items()}\n",
        "#     }\n",
        "    \n",
        "#     prompt = f\"\"\"\n",
        "#     You are an expert data architecture diagram designer. Create a professional-grade Mermaid flowchart diagram for a Great Expectations data quality system.\n",
        "    \n",
        "#     System Context:\n",
        "#     - Data Assets: {architecture_context['data_assets']}\n",
        "#     - Data Sources: {architecture_context['datasources']}\n",
        "#     - Schemas: {architecture_context['schemas']}\n",
        "#     - Expectation Suites: {architecture_context['expectation_suites']}\n",
        "#     - Total Columns: {architecture_context['total_columns']}\n",
        "#     - Validation Runs: {architecture_context['validation_runs']}\n",
        "    \n",
        "#     Asset Details:\n",
        "#     {json.dumps(architecture_context['asset_details'], indent=2)}\n",
        "    \n",
        "#     Create a comprehensive, professional Mermaid flowchart that follows this structure:\n",
        "    \n",
        "#     1. **People Layer**: Data engineers, analysts, stakeholders interacting with the system\n",
        "#     2. **Data Ingestion Layer**: File uploads, database connections, API integrations\n",
        "#     3. **Data Processing Layer**: Data validation, transformation, quality checks\n",
        "#     4. **Great Expectations Engine**: Expectation suites, validators, checkpoints\n",
        "#     5. **AI Analysis Layer**: Ollama integration, intelligent insights, recommendations\n",
        "#     6. **Output Layer**: Reports, dashboards, alerts, documentation\n",
        "#     7. **Data Persistence**: Storage of expectations, results, documentation\n",
        "    \n",
        "#     Requirements:\n",
        "#     - Use subgraphs to organize logical layers\n",
        "#     - Include detailed node labels with specific technologies/processes\n",
        "#     - Show data flow with directional arrows and descriptive labels\n",
        "#     - Add styling with different colors for each layer\n",
        "#     - Include storage/persistence components with dotted lines\n",
        "#     - Make it enterprise-grade and professional\n",
        "#     - Use specific technology names (Ollama Cloud, gpt-oss:20b, Pandas, etc.)\n",
        "#     - Include actual component names from the system context\n",
        "    \n",
        "#     Return ONLY the Mermaid code, no explanations or markdown formatting.\n",
        "#     \"\"\"\n",
        "    \n",
        "#     print(\"Generating high-quality Mermaid architecture diagram with Ollama AI...\")\n",
        "#     ai_response = analyzer.ollama_infer(prompt)\n",
        "    \n",
        "#     if ai_response is None:\n",
        "#         print(\"Ollama AI unavailable, using enhanced fallback Mermaid diagram...\")\n",
        "#         ai_response = generate_enhanced_fallback_mermaid_architecture(architecture_context)\n",
        "    \n",
        "#     return ai_response.strip()\n",
        "\n",
        "# def generate_enhanced_fallback_mermaid_architecture(architecture_context):\n",
        "#     \"\"\"Generate enhanced fallback Mermaid architecture diagram\"\"\"\n",
        "    \n",
        "#     # Get actual asset names for more realistic diagram\n",
        "#     assets = architecture_context['data_assets']\n",
        "#     suites = architecture_context['expectation_suites']\n",
        "#     datasources = architecture_context['datasources']\n",
        "    \n",
        "#     asset1 = assets[0] if assets else \"nyc_taxi_data\"\n",
        "#     asset2 = assets[1] if len(assets) > 1 else \"users_table\"\n",
        "#     suite1 = suites[0] if suites else \"onboarding_suite\"\n",
        "#     suite2 = suites[1] if len(suites) > 1 else \"quality_suite\"\n",
        "#     datasource1 = datasources[0] if datasources else \"postgres_db\"\n",
        "    \n",
        "#     fallback_diagram = f\"\"\"graph TB\n",
        "#     subgraph \"People Layer\"\n",
        "#         A[Data Engineer] -->|Uploads Data| B[Data Source Selection]\n",
        "#         A -->|Natural Language Query| C[Query Input Interface]\n",
        "#         D[Data Analyst] -->|Reviews Results| E[Quality Dashboard]\n",
        "#     end\n",
        "\n",
        "#     subgraph \"Data Ingestion Layer\"\n",
        "#         B -->|CSV Upload| F[File Upload Handler]\n",
        "#         B -->|Database Connection| G[Database Browser]\n",
        "#         F --> H[Data Preview & Schema Detection]\n",
        "#         G --> H\n",
        "#         H -->|DataFrame Creation| I[Pandas DataProcessor]\n",
        "#     end\n",
        "\n",
        "#     subgraph \"Great Expectations Engine\"\n",
        "#         I -->|Create Batch| J[GX Batch Definition]\n",
        "#         J -->|Load Data| K[Data Validator]\n",
        "#         K -->|Execute Expectations| L[Expectation Suite Runner]\n",
        "#         L -->|{suite1}| M[Quality Validation Engine]\n",
        "#         L -->|{suite2}| M\n",
        "#     end\n",
        "\n",
        "#     subgraph \"AI Analysis Layer\"\n",
        "#         M -->|Validation Results| N[Ollama Cloud API]\n",
        "#         N -->|Model: gpt-oss:20b| O[AI Quality Analyzer]\n",
        "#         O -->|Generate Insights| P[Intelligent Recommendations]\n",
        "#         P -->|Quality Metrics| Q[AI-Powered Reports]\n",
        "#     end\n",
        "\n",
        "#     subgraph \"Output Layer\"\n",
        "#         Q -->|Build Reports| R[Data Docs Builder]\n",
        "#         R -->|HTML Generation| S[Interactive Reports]\n",
        "#         S -->|file:// URL| T[Browser Display]\n",
        "#         Q -->|Success/Failure| U[Status Dashboard]\n",
        "#         U -->|Real-time Metrics| V[Streamlit UI]\n",
        "#         E -->|Monitor Quality| V\n",
        "#     end\n",
        "\n",
        "#     subgraph \"Data Persistence\"\n",
        "#         M -.->|Save Suite| W[(Expectations Store)]\n",
        "#         L -.->|Save Results| X[(Validations Store)]\n",
        "#         R -.->|Write HTML| Y[(Data Docs Store)]\n",
        "#         O -.->|Save Analysis| Z[(AI Insights Store)]\n",
        "#     end\n",
        "\n",
        "#     subgraph \"Monitored Data Assets\"\n",
        "#         AA[{asset1}<br/>Columns: {architecture_context['total_columns']}]\n",
        "#         BB[{asset2}<br/>Schema: {architecture_context['schemas'][0] if architecture_context['schemas'] else 'public'}]\n",
        "#         CC[{datasource1}<br/>Database Connection]\n",
        "#     end\n",
        "\n",
        "#     CC --> AA\n",
        "#     CC --> BB\n",
        "#     AA --> I\n",
        "#     BB --> I\n",
        "\n",
        "#     style A fill:#e1f5ff\n",
        "#     style D fill:#e1f5ff\n",
        "#     style N fill:#fff3e0\n",
        "#     style M fill:#f3e5f5\n",
        "#     style R fill:#e8f5e9\n",
        "#     style W fill:#fce4ec\n",
        "#     style X fill:#fce4ec\n",
        "#     style Y fill:#fce4ec\n",
        "#     style Z fill:#fce4ec\n",
        "#     style AA fill:#e8f5e9\n",
        "#     style BB fill:#e8f5e9\n",
        "#     style CC fill:#e8f5e9\"\"\"\n",
        "    \n",
        "#     return fallback_diagram\n",
        "\n",
        "# def generate_mermaid_data_model(data_catalog, analyzer):\n",
        "#     \"\"\"Generate Mermaid data model diagram using Ollama AI\"\"\"\n",
        "    \n",
        "#     # Prepare data model context\n",
        "#     data_model_context = {\n",
        "#         'assets': {name: {\n",
        "#             'columns': len(info['columns']),\n",
        "#             'suites': info['expectation_suites'],\n",
        "#             'type': info['type'],\n",
        "#             'table': info['table_name']\n",
        "#         } for name, info in data_catalog['data_assets'].items()},\n",
        "#         'suites': {name: {\n",
        "#             'expectations': info['quality_metrics']['total_expectations'],\n",
        "#             'assets': info['data_assets']\n",
        "#         } for name, info in data_catalog['expectation_suites'].items()}\n",
        "#     }\n",
        "    \n",
        "#     prompt = f\"\"\"\n",
        "#     You are a data modeling expert. Generate a Mermaid ER diagram for a Great Expectations data quality system.\n",
        "    \n",
        "#     Data Model Context:\n",
        "#     {json.dumps(data_model_context, indent=2)}\n",
        "    \n",
        "#     Create a comprehensive Mermaid ER diagram that shows:\n",
        "#     1. Data assets as entities with their attributes\n",
        "#     2. Expectation suites as entities\n",
        "#     3. Relationships between data assets and expectation suites\n",
        "#     4. Column information for each data asset\n",
        "#     5. Quality metrics and validation results\n",
        "#     6. Proper entity-relationship notation\n",
        "    \n",
        "#     Use proper Mermaid ER syntax with:\n",
        "#     - Entity definitions with attributes\n",
        "#     - Relationship lines with cardinality\n",
        "#     - Clear labeling\n",
        "#     - Appropriate styling\n",
        "#     - Logical grouping\n",
        "    \n",
        "#     Return ONLY the Mermaid code, no explanations or markdown formatting.\n",
        "#     \"\"\"\n",
        "    \n",
        "#     print(\"Generating Mermaid data model diagram with Ollama AI...\")\n",
        "#     ai_response = analyzer.ollama_infer(prompt)\n",
        "    \n",
        "#     if ai_response is None:\n",
        "#         print(\"Ollama AI unavailable, using fallback Mermaid data model...\")\n",
        "#         ai_response = generate_fallback_mermaid_data_model(data_model_context)\n",
        "    \n",
        "#     return ai_response.strip()\n",
        "\n",
        "# def generate_fallback_mermaid_data_model(data_model_context):\n",
        "#     \"\"\"Generate fallback Mermaid data model diagram\"\"\"\n",
        "    \n",
        "#     fallback_diagram = f\"\"\"erDiagram\n",
        "#     DATA_ASSET {{\n",
        "#         string asset_name PK\n",
        "#         string asset_type\n",
        "#         string table_name\n",
        "#         string schema_name\n",
        "#         string datasource\n",
        "#         int column_count\n",
        "#         int suite_count\n",
        "#     }}\n",
        "    \n",
        "#     EXPECTATION_SUITE {{\n",
        "#         string suite_name PK\n",
        "#         int total_expectations\n",
        "#         float success_rate\n",
        "#         int exceptions\n",
        "#         string data_assets\n",
        "#     }}\n",
        "    \n",
        "#     COLUMN {{\n",
        "#         string column_name PK\n",
        "#         string asset_name FK\n",
        "#         int expectation_count\n",
        "#         float success_rate\n",
        "#         int exceptions\n",
        "#     }}\n",
        "    \n",
        "#     VALIDATION_RUN {{\n",
        "#         string run_id PK\n",
        "#         string asset_name FK\n",
        "#         string suite_name FK\n",
        "#         datetime timestamp\n",
        "#         int expectation_count\n",
        "#         float success_rate\n",
        "#     }}\n",
        "    \n",
        "#     DATA_ASSET ||--o{{ COLUMN : contains\n",
        "#     DATA_ASSET ||--o{{ VALIDATION_RUN : validates\n",
        "#     EXPECTATION_SUITE ||--o{{ VALIDATION_RUN : executes\n",
        "#     DATA_ASSET o--o{{ EXPECTATION_SUITE : monitored_by\"\"\"\n",
        "    \n",
        "#     return fallback_diagram\n",
        "\n",
        "# def render_mermaid_diagram(mermaid_code, diagram_name=\"diagram\"):\n",
        "#     \"\"\"Render Mermaid diagram in Jupyter notebook\"\"\"\n",
        "    \n",
        "#     # Create HTML with Mermaid.js\n",
        "#     html_content = \"\"\"\n",
        "#     <div class=\"mermaid\">\n",
        "#     \"\"\" + mermaid_code + \"\"\"\n",
        "    \n",
        "    \n",
        "#     <script src=\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>\n",
        "#     <script>\n",
        "#         mermaid.initialize({\"startOnLoad\": true, \"theme\": \"default\"});\n",
        "#     </script>\n",
        "#     \"\"\"\n",
        "    \n",
        "#     return HTML(html_content)\n",
        "\n",
        "# def save_mermaid_diagram(mermaid_code, filename):\n",
        "#     \"\"\"Save Mermaid diagram to file\"\"\"\n",
        "    \n",
        "#     file_path = Path(\"notebooks/great_expectations\") / filename\n",
        "    \n",
        "#     with open(file_path, 'w', encoding='utf-8') as f:\n",
        "#         f.write(mermaid_code)\n",
        "    \n",
        "#     print(f\"Mermaid diagram saved to: {file_path}\")\n",
        "#     return file_path\n",
        "\n",
        "# # Generate Mermaid diagrams\n",
        "# print(\"Generating Mermaid diagrams with Ollama AI...\")\n",
        "\n",
        "# if 'data_catalog' in locals():\n",
        "#     # Generate architecture diagram\n",
        "#     print(\"Creating architecture diagram...\")\n",
        "#     architecture_mermaid = generate_mermaid_architecture(data_catalog, analyzer)\n",
        "    \n",
        "#     # Save architecture diagram\n",
        "#     arch_file = save_mermaid_diagram(architecture_mermaid, \"architecture_diagram.mmd\")\n",
        "    \n",
        "#     # Render architecture diagram\n",
        "#     print(\"Rendering architecture diagram...\")\n",
        "#     # display(render_mermaid_diagram(architecture_mermaid, \"architecture\"))\n",
        "    \n",
        "#     # Generate data model diagram\n",
        "#     print(\"Creating data model diagram...\")\n",
        "#     data_model_mermaid = generate_mermaid_data_model(data_catalog, analyzer)\n",
        "    \n",
        "#     # Save data model diagram\n",
        "#     model_file = save_mermaid_diagram(data_model_mermaid, \"data_model_diagram.mmd\")\n",
        "    \n",
        "#     # Render data model diagram\n",
        "#     print(\"Rendering data model diagram...\")\n",
        "#     # display(render_mermaid_diagram(data_model_mermaid, \"data_model\"))\n",
        "    \n",
        "#     # Create combined markdown report with diagrams\n",
        "#     combined_report = f\"\"\"# Data Architecture and Model Diagrams\n",
        "\n",
        "# ## Architecture Diagram\n",
        "\n",
        "# ```mermaid\n",
        "# {architecture_mermaid}\n",
        "# ```\n",
        "\n",
        "# ## Data Model Diagram\n",
        "\n",
        "# ```mermaid\n",
        "# {data_model_mermaid}\n",
        "# ```\n",
        "\n",
        "# ---\n",
        "# *Diagrams generated by Ollama AI and Great Expectations Validation Analysis system*\n",
        "# \"\"\"\n",
        "    \n",
        "#     # Save combined report\n",
        "#     combined_report_path = Path(\"notebooks/great_expectations\") / \"architecture_diagrams_report.md\"\n",
        "#     with open(combined_report_path, 'w', encoding='utf-8') as f:\n",
        "#         f.write(combined_report)\n",
        "    \n",
        "#     print(f\"Combined diagrams report saved to: {combined_report_path}\")\n",
        "    \n",
        "#     print(\"Mermaid diagram generation complete!\")\n",
        "#     print(f\"Architecture diagram: {arch_file}\")\n",
        "#     print(f\"Data model diagram: {model_file}\")\n",
        "#     print(f\"Combined report: {combined_report_path}\")\n",
        "    \n",
        "# else:\n",
        "#     print(\"Data catalog not available. Please run the data catalog generation cell first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from pathlib import Path\n",
        "# import shutil\n",
        "\n",
        "# # Define the outputs directory path\n",
        "# outputs_dir = Path(\"notebooks/great_expectations/outputs\")\n",
        "# outputs_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# # List of files to move into the outputs directory\n",
        "# files_to_move = [\n",
        "#     \"architecture_diagram.mmd\",\n",
        "#     \"data_model_diagram.mmd\",\n",
        "#     \"architecture_diagrams_report.md\",\n",
        "#     \"validation_analysis_report.md\",\n",
        "#     \"validation_analysis_report_enhanced.md\",\n",
        "#     \"data_catalog_report.md\",\n",
        "#     \"data_catalog.json\",\n",
        "#     \"validation_analysis_report_enhanced.pdf\"\n",
        "# ]\n",
        "\n",
        "# # Move each file to the outputs directory if it exists\n",
        "# for filename in files_to_move:\n",
        "#     src = Path(\"notebooks/great_expectations\") / filename\n",
        "#     dst = outputs_dir / filename\n",
        "#     if src.exists():\n",
        "#         shutil.move(str(src), str(dst))\n",
        "#         print(f\"Moved {src} to {dst}\")\n",
        "#     else:\n",
        "#         print(f\"File not found, skipping: {src}\")\n",
        "\n",
        "# print(\"All specified files have been moved to the outputs directory.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "GREAT EXPECTATIONS VALIDATION ANALYSIS COMPLETE!\n",
            "================================================================================\n",
            "Analysis completed at: 2025-10-07 14:40:32\n",
            "Total execution time: 00:00:07\n",
            "Total duration: 7.80 seconds\n",
            "================================================================================\n",
            "📈 Performance rating: Excellent - Very fast execution\n",
            "================================================================================\n",
            "📄 Generated Reports and Files:\n",
            "   • validation_analysis_report.md - Basic analysis report\n",
            "   • validation_analysis_report_enhanced.md - Enhanced report with data catalog\n",
            "   • validation_analysis_report_enhanced.pdf - Professional PDF report\n",
            "   • data_catalog_report.md - Data catalog documentation\n",
            "   • data_catalog.json - Structured data catalog\n",
            "   • architecture_diagram.mmd - System architecture diagram\n",
            "   • data_model_diagram.mmd - Data model ER diagram\n",
            "   • enhanced_data_model_diagram_fixed.mmd - Enhanced data model\n",
            "   • architecture_diagrams_report.md - Combined diagrams report\n",
            "   • comprehensive_validation_analysis_report.md - Complete analysis report\n",
            "================================================================================\n",
            "All analysis tasks completed successfully!\n",
            "Ready for stakeholder review and presentation\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Analysis Completion and Timing Summary\n",
        "end_time = time.time()\n",
        "analysis_end = datetime.now()\n",
        "total_duration = end_time - start_time\n",
        "\n",
        "# Calculate timing breakdown\n",
        "hours = int(total_duration // 3600)\n",
        "minutes = int((total_duration % 3600) // 60)\n",
        "seconds = int(total_duration % 60)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"GREAT EXPECTATIONS VALIDATION ANALYSIS COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Analysis completed at: {analysis_end.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"Total execution time: {hours:02d}:{minutes:02d}:{seconds:02d}\")\n",
        "print(f\"Total duration: {total_duration:.2f} seconds\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Performance summary\n",
        "if total_duration < 60:\n",
        "    performance = \"Excellent - Very fast execution\"\n",
        "elif total_duration < 300:\n",
        "    performance = \"Good - Fast execution\"\n",
        "elif total_duration < 600:\n",
        "    performance = \"Moderate - Reasonable execution time\"\n",
        "else:\n",
        "    performance = \"Slow - Consider optimization\"\n",
        "\n",
        "print(f\"📈 Performance rating: {performance}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Summary of outputs generated\n",
        "print(\"📄 Generated Reports and Files:\")\n",
        "print(\"   • validation_analysis_report.md - Basic analysis report\")\n",
        "print(\"   • validation_analysis_report_enhanced.md - Enhanced report with data catalog\")\n",
        "print(\"   • validation_analysis_report_enhanced.pdf - Professional PDF report\")\n",
        "print(\"   • data_catalog_report.md - Data catalog documentation\")\n",
        "print(\"   • data_catalog.json - Structured data catalog\")\n",
        "print(\"   • architecture_diagram.mmd - System architecture diagram\")\n",
        "print(\"   • data_model_diagram.mmd - Data model ER diagram\")\n",
        "print(\"   • enhanced_data_model_diagram_fixed.mmd - Enhanced data model\")\n",
        "print(\"   • architecture_diagrams_report.md - Combined diagrams report\")\n",
        "print(\"   • comprehensive_validation_analysis_report.md - Complete analysis report\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"All analysis tasks completed successfully!\")\n",
        "print(\"Ready for stakeholder review and presentation\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate AI-Powered Executive Summary for Professional PDF Report\n",
        "print(\"🤖 Generating AI-powered executive summary for professional PDF report...\")\n",
        "\n",
        "def generate_ai_executive_summary(df, quality_metrics, data_summary, analyzer):\n",
        "    \"\"\"Generate AI-powered executive summary following professional standards\"\"\"\n",
        "    \n",
        "    # Extract key metrics for summary\n",
        "    overall_success = quality_metrics['overall_success_rate']\n",
        "    exception_rate = quality_metrics['exception_rate']\n",
        "    total_expectations = len(df)\n",
        "    \n",
        "    # Get top failing expectation types for summary\n",
        "    failing_types = quality_metrics['type_metrics'].nsmallest(3, 'success_rate')\n",
        "    \n",
        "    # Generate AI-powered executive summary\n",
        "    executive_summary_prompt = f\"\"\"\n",
        "    You are a senior data quality consultant writing an executive summary for a Great Expectations validation analysis report.\n",
        "    \n",
        "    Data Quality Context:\n",
        "    - Total Expectations: {total_expectations}\n",
        "    - Overall Success Rate: {overall_success:.2%}\n",
        "    - Exception Rate: {exception_rate:.2%}\n",
        "    - Expectation Types: {data_summary['expectation_types']}\n",
        "    - Validation Suites: {data_summary['suite_count']}\n",
        "    - Critical Issues: {len(failing_types[failing_types['success_rate'] < 0.8])} expectation types below 80% success rate\n",
        "    \n",
        "    Top Failing Expectation Types:\n",
        "    {failing_types[['total_expectations', 'successful_expectations', 'success_rate', 'exceptions']].to_dict()}\n",
        "    \n",
        "    Write a professional executive summary that follows these guidelines:\n",
        "    \n",
        "    1. **Problem Statement**: Define the data quality challenge being addressed\n",
        "    2. **Solution Approach**: Explain how Great Expectations addresses the problem\n",
        "    3. **Key Findings**: Present the most critical insights from the analysis\n",
        "    4. **Business Impact**: Highlight the value and benefits of the data quality program\n",
        "    5. **Call to Action**: Provide clear next steps for decision-makers\n",
        "    \n",
        "    Requirements:\n",
        "    - Write for C-level executives and decision-makers\n",
        "    - Use clear, simple language (15-year-old reading level)\n",
        "    - Keep to 500-800 words maximum\n",
        "    - Focus on business value and strategic importance\n",
        "    - Include specific metrics and actionable recommendations\n",
        "    - Avoid technical jargon\n",
        "    - Create urgency for immediate action\n",
        "    \n",
        "    Format as a professional executive summary suitable for a board presentation.\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"🤖 Generating AI-powered executive summary...\")\n",
        "    ai_executive_summary = analyzer.ollama_infer(executive_summary_prompt)\n",
        "    \n",
        "    # Fallback executive summary if AI is unavailable\n",
        "    if ai_executive_summary is None:\n",
        "        print(\"⚠️ AI unavailable, using fallback executive summary...\")\n",
        "        ai_executive_summary = f\"\"\"\n",
        "        ## Executive Summary\n",
        "        \n",
        "        This Great Expectations validation analysis reveals critical insights into our data quality program's performance across {data_summary['suite_count']} validation suites monitoring {total_expectations} data quality expectations.\n",
        "        \n",
        "        **Key Findings:**\n",
        "        Our data quality program demonstrates strong overall performance with a {overall_success:.2%} success rate, indicating robust data governance processes. However, {len(failing_types[failing_types['success_rate'] < 0.8])} expectation types require immediate attention due to success rates below 80%.\n",
        "        \n",
        "        **Business Impact:**\n",
        "        The current data quality metrics suggest reliable data for most analytical workloads, but specific expectation failures could impact downstream analytics and reporting accuracy. Immediate remediation of failing expectations is recommended to maintain data trust and prevent potential business impact.\n",
        "        \n",
        "        **Recommendation:**\n",
        "        Prioritize fixing expectation types with success rates below 80% to ensure comprehensive data quality coverage and maintain stakeholder confidence in our data assets.\n",
        "        \"\"\"\n",
        "    \n",
        "    return ai_executive_summary\n",
        "\n",
        "# Generate AI executive summary\n",
        "ai_executive_summary = generate_ai_executive_summary(df, quality_metrics, data_summary, analyzer)\n",
        "\n",
        "# Create professional report with AI executive summary\n",
        "def create_professional_report_with_ai_summary(df, quality_metrics, ai_insights, data_summary, ai_executive_summary, data_catalog=None):\n",
        "    \"\"\"Create professional report with AI-generated executive summary\"\"\"\n",
        "    \n",
        "    # Extract key metrics\n",
        "    overall_success = quality_metrics['overall_success_rate']\n",
        "    exception_rate = quality_metrics['exception_rate']\n",
        "    total_expectations = len(df)\n",
        "    failing_types = quality_metrics['type_metrics'].nsmallest(3, 'success_rate')\n",
        "    \n",
        "    report = f\"\"\"# Great Expectations Validation Analysis Report\n",
        "\n",
        "**Generated on:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n",
        "**Analysis Period:** {data_summary['date_range']}\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "{ai_executive_summary}\n",
        "\n",
        "## Critical Findings\n",
        "\n",
        "### Top Issues Requiring Attention\n",
        "\"\"\"\n",
        "    \n",
        "    # Add critical issues\n",
        "    for idx, (exp_type, metrics) in enumerate(failing_types.iterrows(), 1):\n",
        "        if metrics['success_rate'] < 0.8:\n",
        "            report += f\"{idx}. **{exp_type}**: {metrics['success_rate']:.1%} success rate ({metrics['successful_expectations']}/{metrics['total_expectations']} expectations)\\n\"\n",
        "    \n",
        "    report += f\"\"\"\n",
        "## Data Quality Analysis\n",
        "\n",
        "### Overall Performance Metrics\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Total Expectations | {total_expectations} |\n",
        "| Overall Success Rate | {overall_success:.2%} |\n",
        "| Exception Rate | {exception_rate:.2%} |\n",
        "| Expectation Types | {data_summary['expectation_types']} |\n",
        "| Validation Suites | {data_summary['suite_count']} |\n",
        "\n",
        "### Suite Performance\n",
        "\n",
        "| Suite Name | Expectations | Success Rate | Exceptions |\n",
        "|------------|-------------|--------------|------------|\n",
        "\"\"\"\n",
        "    \n",
        "    # Add suite metrics\n",
        "    for suite, metrics in quality_metrics['suite_metrics'].iterrows():\n",
        "        report += f\"| {suite} | {metrics['total_expectations']} | {metrics['success_rate']:.2%} | {metrics['exceptions']} |\\n\"\n",
        "    \n",
        "    report += f\"\"\"\n",
        "### Expectation Type Performance\n",
        "\n",
        "| Expectation Type | Count | Success Rate | Exceptions |\n",
        "|------------------|-------|--------------|------------|\n",
        "\"\"\"\n",
        "    \n",
        "    # Add type metrics - FIXED: Use correct column name 'total_expectations' instead of 'count'\n",
        "    for exp_type, metrics in quality_metrics['type_metrics'].iterrows():\n",
        "        report += f\"| {exp_type} | {metrics['total_expectations']} | {metrics['success_rate']:.2%} | {metrics['exceptions']} |\\n\"\n",
        "    \n",
        "    report += f\"\"\"\n",
        "## AI-Powered Analysis\n",
        "\n",
        "{ai_insights}\n",
        "\n",
        "## Data Catalog Summary\n",
        "\n",
        "### Data Assets Overview\n",
        "\n",
        "| Asset Name | Type | Table | Schema | Datasource | Columns | Suites |\n",
        "|------------|------|-------|--------|------------|---------|--------|\n",
        "\"\"\"\n",
        "    \n",
        "    # Add data catalog information\n",
        "    if data_catalog:\n",
        "        for asset_name, asset_info in data_catalog['data_assets'].items():\n",
        "            report += f\"| {asset_name} | {asset_info['type']} | {asset_info['table_name']} | {asset_info['schema_name']} | {asset_info['datasource']} | {len(asset_info['columns'])} | {len(asset_info['expectation_suites'])} |\\n\"\n",
        "    \n",
        "    report += f\"\"\"\n",
        "### Expectation Suites Overview\n",
        "\n",
        "| Suite Name | Total Expectations | Success Rate | Exceptions | Data Assets |\n",
        "|------------|-------------------|--------------|------------|-------------|\n",
        "\"\"\"\n",
        "    \n",
        "    # Add expectation suites information\n",
        "    if data_catalog:\n",
        "        for suite_name, suite_info in data_catalog['expectation_suites'].items():\n",
        "            report += f\"| {suite_name} | {suite_info['quality_metrics']['total_expectations']} | {suite_info['quality_metrics']['success_rate']:.2%} | {suite_info['quality_metrics']['exceptions']} | {len(suite_info['data_assets'])} |\\n\"\n",
        "    \n",
        "    report += f\"\"\"\n",
        "## Recommendations\n",
        "\n",
        "Based on the analysis, the following actions are recommended:\n",
        "\n",
        "1. **Immediate Actions**: Address expectation types with success rates below 80%\n",
        "2. **Monitoring**: Implement daily monitoring for critical data assets\n",
        "3. **Expectation Review**: Review and update failing expectation configurations\n",
        "4. **Process Improvement**: Establish data quality governance processes\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "- **Analysis Engine**: Great Expectations v0.18.22\n",
        "- **AI Analysis**: Ollama LLM (gpt-oss:20b)\n",
        "- **Data Source**: Validation results from BirdiDQ/gx/uncommitted/validations\n",
        "- **Report Generated**: {datetime.now().isoformat()}\n",
        "\n",
        "---\n",
        "*This report was automatically generated by the Great Expectations Validation Analysis system.*\n",
        "\"\"\"\n",
        "    \n",
        "    return report\n",
        "\n",
        "# Generate the professional report with AI executive summary\n",
        "professional_report = create_professional_report_with_ai_summary(df, quality_metrics, ai_insights, data_summary, ai_executive_summary, data_catalog)\n",
        "\n",
        "# Save professional markdown report\n",
        "professional_report_path = Path(\"notebooks/great_expectations\") / \"validation_analysis_report_ai_executive.md\"\n",
        "with open(professional_report_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(professional_report)\n",
        "\n",
        "print(f\"📄 Professional markdown report with AI executive summary saved to: {professional_report_path}\")\n",
        "\n",
        "# Generate professional PDF report\n",
        "print(\"📄 Generating professional PDF report with AI executive summary...\")\n",
        "professional_pdf_path = generate_pdf_report(professional_report, \"validation_analysis_report_ai_executive.pdf\")\n",
        "\n",
        "if professional_pdf_path:\n",
        "    print(f\"✅ Professional PDF report with AI executive summary generated successfully!\")\n",
        "    print(f\"📁 Professional PDF location: {professional_pdf_path}\")\n",
        "    print(\"\\n🎯 This PDF features:\")\n",
        "    print(\"   • AI-generated executive summary for C-level executives\")\n",
        "    print(\"   • Clean formatting without grey backgrounds\")\n",
        "    print(\"   • Professional structure following best practices\")\n",
        "    print(\"   • Clear call-to-action and business impact focus\")\n",
        "    print(\"   • Strategic recommendations for decision-makers\")\n",
        "else:\n",
        "    print(\"⚠️ Professional PDF generation failed - check error messages above\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
